{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#d35400'> Lab 7 | Dimensionality Reduction & Unsupervised Machine Learning </font>\n",
    "Welcome to Lab 7! This lab analyzes 63,542 emails. We convert these emails into raw text using a feature matrix of a \"bag of words\" model. Each column of the feature matrix corresponds to one word, each row corresponds to one email, and the entry stores the number of times that word was found in the email. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"coffee_dog.jpg\" alt=\"Alt Text\", width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 1 </font> | Data Loading\n",
    "In this section, we load a dataset of emails that are stored as individual JSON files within a directory called \"email json\". Each file follows the naming format \"message XXXXX.json\". The goal is to write a python script that locates and loads all JSON files, parsing them into a list of dictionaries. The list is then converted into a Pandas Dataframe.\n",
    "\n",
    "- [x] Extract the provided email json.zip ﬁle. It should create a directory called “email json”. Each email is stored as a separate JSON document with a name in the format “message XXXXX.json”.\n",
    "- [x] Remember when I introduced the Titanic data set in the ﬁrst lab and I asked the class to import the ﬁle by hand? That practice will pay oﬀ here. Write some code to ﬁnd and load all of the JSON documents. You should have a list of dicts when done (Hint: Review the built-in Python json library).\n",
    "- [x] Convert the list of dicts into a Pandas DataFrame. The DataFrame should have 5 columns and 63,452 rows (Hint: use the DataFrame.from records() or DataFrame.from dict() functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Extracting the Zip File </font>\n",
    "We start off by extracting the zip file. We then store the `.json` files in a directory named `email_json`. We know that the JSON files come with a name in the format `messageXXXXX.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the os library\n",
    "import os\n",
    "\n",
    "# listing out the files in the directory\n",
    "directory = r\"C:\\GitHub\\DataScienceMachineLearning\\wk_09\\lab\\email_json\"\n",
    "files = os.listdir(directory)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Loading the JSON Documents </font>\n",
    "Next, we load all the JSON documents and produce a list of dicts using the Python JSON library. We achieve this by going into every JSON document and parsing them into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the json library\n",
    "import json\n",
    "\n",
    "# importing the tqdm library\n",
    "from tqdm import tqdm\n",
    "\n",
    "# setting up the dictionary\n",
    "email_dictionary = []\n",
    "\n",
    "for filename in tqdm(files):\n",
    "    file_path = os.path.join(\"email_json\", filename)\n",
    "    with open(file_path, 'r') as file:\n",
    "        email_contents = json.load(file)\n",
    "        email_dictionary.append(email_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Creating a Pandas Dictionary </font>\n",
    "We now convert the list of dictionaries into a pandas data frame. The data frame is expected to have 5 columns and 63,452 rows. We achieve this using `DataFrame.fromrecords()` and `DataFrame.fromdict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# importing the ipython display\n",
    "from IPython.display import display\n",
    "\n",
    "# converting the dictionary into a data frame\n",
    "email_df = pd.DataFrame.from_records(email_dictionary)\n",
    "\n",
    "# confirming the shape of the data frame\n",
    "display(email_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 2 </font> | Extracting the Features\n",
    "In this section, we prepare the email message bodies for analysis. We do this by converting the raw text into a numerical format using a feature matrix. To achieve this, we use `Scikit-learn`'s `CountVectorizer` which records only the presence or absence of a word.\n",
    "\n",
    "- [x] Use Scikit Learn’s CountVectorizer class with the binary=True ﬂag to create a feature matrix from the message bodies. When doing this set min df=10 to exclude any words that do not appear in at least 10 emails. The “bodies” column of the DataFrame can be used as a list of strings and passed directly into the ﬁt transform() method of the\n",
    "CountVectorizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Preparation for Analysis </font>\n",
    "Here, we extract the email message bodies to process the text data and convert it into a numerical format. What we end up getting is a matrix of 1's and 0's, where each row represents an email and each column represents a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# creating a count vectorizer object\n",
    "count_vectorizer = CountVectorizer(binary=True, min_df=10)\n",
    "\n",
    "# using fit_transform to create a feature matrix\n",
    "feature_matrix = count_vectorizer.fit_transform(email_df['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Visualizing Feature Matrix </font>\n",
    "Lastly, we would like to check out the feature matrix. We do this by converting it into a Pandas data frame. This just allows us to visualize and check if what we're doing is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the feature matrix\n",
    "features = count_vectorizer.get_feature_names_out()\n",
    "feature_df = pd.DataFrame(feature_matrix.toarray(), columns=features)\n",
    "display(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 3 </font> | Dimensionality Reduction / Visualization\n",
    "We now focus on reduce the dimensionality of the large feature matrix, we use `TruncatedSVD` from Scikit-learn with the `fit_transform()` function to create a new matrix with 10 components. We then plot the explained variance ratios of these components and create a scatter plot using the two components and a second scatter plot where the points are colored based on the category.\n",
    "\n",
    "- [x] This matrix has too much information to be directly useful. Use the ﬁt transform() function of Scikit Learn’s TruncatedSVD class to transform the original feature matrix into a new feature matrix with 10 columns (variables or components). Scikit Learn’s TruncatedSVD method is similar to its PCA method, but it works with sparse matrices.\n",
    "- [x] Plot the explained variance ratios of the components (Hint: use the explained variance ratio property of the TruncatedSVD class)\n",
    "- [x] Create a scatter plot using the two components with the highest explained variance ratios (Hint: plt.scatter(proj matrix[:, i], proj matrix[:, j])).\n",
    "- [x] Create a second scatter plot using the same two components. This time, color the points based on the category column of the DataFrame. All spam messages should be one color; all ham messages should be a second color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Converting the Feature Matrix </font>\n",
    "The current matrix we have has too much information to be useful. We use `fit_transform()` that comes from Scikit Learn's `TruncatedSVD` class to transform the original feature matrix into a new one with 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the TruncatedSVD library\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# initializing the TruncatedSVD object\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "\n",
    "# applying fit_transform to reduce the feature matrix to 10 components\n",
    "svd_10 = svd.fit_transform(feature_matrix)\n",
    "\n",
    "# printing out the new shape\n",
    "display(svd_10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting Explained Variance </font>\n",
    "Next, we plot the explained variance ratios of the components. We use the `explained_variance_ratio_property` of the TruncatedSVD class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting the plot using explained variance\n",
    "plt.plot(svd.explained_variance_ratio_)\n",
    "plt.title(\"Explained Variance Ratio by Component\")\n",
    "\n",
    "# labeling the x and y axes\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "\n",
    "# setting the limits from 0 to 10\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "# saving the plot\n",
    "plt.savefig(r'C:\\GitHub\\DataScienceMachineLearning\\wk_09\\plots\\explainedvarianceratio.png', dpi=300)\n",
    "\n",
    "# graphing out the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting the First Scatter Plot </font>\n",
    "Next, we create a scatter plot using the two components with the highest explained variance ratios. We use example code such as `plt.scatter(proj_matrix[:,i], proj_matrix[:,j])`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# finding the highest explained variance ratios\n",
    "top_two = np.argsort(svd.explained_variance_ratio_)[-2:][::-1]\n",
    "\n",
    "# extracting the highest values\n",
    "i, j = top_two[0],top_two[1]\n",
    "\n",
    "# plotting the scatter plot using the two components\n",
    "plt.scatter(svd_10[:, i], svd_10[:,j])\n",
    "\n",
    "# setting the title, x any y axis labels\n",
    "plt.title(\"Scatter Plot of Two Components with Highest Explained Variance Ratios\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "# saving the plot\n",
    "plt.savefig(r'C:\\GitHub\\DataScienceMachineLearning\\wk_09\\plots\\scatterplot.png', dpi=300)\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting the Second Scatter Plot </font>\n",
    "Finally, we create a second scatter plot using the same two components. We color the points based on the category column of the DataFrame. The spam messages should be one color; the ham messages should be a second color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the coloring \n",
    "coloring = email_df['category'].map({'spam':'lightcoral', 'ham':'lightskyblue'})\n",
    "\n",
    "# plotting the scatter plot using the two components\n",
    "plt.scatter(svd_10[:, i], svd_10[:,j], c=coloring, marker='o')\n",
    "\n",
    "# setting the title, x any y axis labels\n",
    "plt.title(\"Scatter Plot of Two Components with Highest Explained Variance Ratios\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "# manually creating legend and and labels for spam\n",
    "spam_handle = plt.Line2D([0], [0], marker='o', color='w', label='Spam',\n",
    "                          markerfacecolor='lightcoral', markersize=10)\n",
    "\n",
    "# manually creating legend and and labels for ham\n",
    "ham_handle = plt.Line2D([0], [0], marker='o', color='w', label='Ham', \n",
    "                        markerfacecolor='lightskyblue', markersize=10)\n",
    "\n",
    "# setting up the legend\n",
    "plt.legend(handles=[spam_handle, ham_handle])\n",
    "\n",
    "# saving the plot\n",
    "plt.savefig(r'C:\\GitHub\\DataScienceMachineLearning\\wk_09\\plots\\scatterplotcolored.png', dpi=300)\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Section 4 </font> | The Clustering Process\n",
    "In this section, we perform clustering of emails based on their features, using a suitable clustering algorithm from Scikit-learn. We perform the necessary transformations, such as dimensionality reduction and we cluster the emails based on the top two components with the highest explained variance. The resulting clusters will be visualized in a scatter plot, with points colored by the cluster labels. \n",
    "\n",
    "- [x] To perform further analysis, we want to cluster the emails. By clustering the emails, each message will be assigned a cluster id (e.g., 0, 1, 2, etc.). There are numerous clustering algorithms, each with strengths and weaknesses. We cannot cover them all in lecture and so begin this section by reviewing the clustering algorithms available in Scikit Learn. Choose a clustering algorithm that you think would be appropriate for\n",
    "this data set.\n",
    "- [x] Cluster the samples using the two SVD components that you determined to have the highest explained ratio AFTER performing any necessary operations to transform them. The clustering algorithm should return a 1D numpy array of cluster labels (e.g., 0, 1, 2, etc.) for each point.\n",
    "- [ ] As in 3.4., create a scatter plot of the two SVD components that you determined to have the highest explained ratio. The clustering algorithm should label the points so that all points in the same cluster have the same cluster id (With two clusters, there should only be two cluster ids). Color the points according to their cluster labels. If the clustering does not look correct, try adjusting the parameters of the clustering algorithm you chose or choose a diﬀerent algorithm.\n",
    "- [ ] Calculate a confusion matrix for the ham / spam labels versus the cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#FF8C00'> Centroid Based and Aggloromerative Hierarchial Clustering </font>\n",
    "We start off by clustering the emails, where each message will be assigned a cluster ID. We choose a clustering algorithm that would be the most appropriate for the data set. We experiment with both centroid based clustering first and then aggloromerative hierarchial clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> The Centroid Based Clustering Approach </font>\n",
    "Starting off with the centroid based clustering approach, we cluster the approach using the two SVD components that we have previously determined. The clustering algorithm will then return a 1D numpy array of cluster labels. We will be using k-means, and before we use k-means, we use the silhouette score to determine the best k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the silhouette score library and KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# storing all the silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# extracting the top two SVD components\n",
    "top2 = svd_10[:, [i, j]] \n",
    "\n",
    "# for looping through the different k values to find the perfect one\n",
    "for k_value in tqdm(range(2, 20)):\n",
    "    kmeans = KMeans(n_clusters=k_value, random_state=42)\n",
    "    kmeans.fit(top2)\n",
    "    score = silhouette_score(top2, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# printing out the silhouette scores\n",
    "print(silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting out the silhouette scores \n",
    "plt.plot(silhouette_scores)\n",
    "\n",
    "# setting the title, x and y labels\n",
    "plt.title(\"Silhouette Scores vs K Values\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "\n",
    "# setting the x axis to show only whole numbers\n",
    "plt.xticks(range(2, 20))\n",
    "plt.xlim(2, 19)\n",
    "\n",
    "# saving the plot\n",
    "plt.savefig(r'C:\\GitHub\\DataScienceMachineLearning\\wk_09\\plots\\silhouettelineplot.png', dpi=300)\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Retrieving the 1D Numpy Array </font>\n",
    "After finding the best value for k, we now cluster the samples and then use the clustering algorithms to return a 1D numpy array of cluster labels for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the kmeans clustering with k = 2\n",
    "kmeans2 = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans2.fit(top2)\n",
    "\n",
    "# retrieving the cluster labels\n",
    "cluster_labels = kmeans2.labels_\n",
    "\n",
    "# printing out the cluster labels\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> Plotting the Centroid Based Scatter Plot </font>\n",
    "Now, we create a scatter plot of the two SVD components. The clustering algorithm labels the points so that all points in the same cluster have the same cluster ID. We color the points according to their cluster labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the scatter plot\n",
    "plt.scatter(svd_10[:, i], svd_10[:,j], c=cluster_labels, marker='o')\n",
    "\n",
    "# plotting the title, x and y labels\n",
    "plt.title(\"Scatter Plot of Top Two SVD Components with k = 2\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "# displaying the legend\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = '#FF8C00'> The Aggloromerative Hierarchial Clustering Approach </font>\n",
    "Starting off with the centroid based clustering approach, we cluster the approach using the two SVD components that we have previously determined. The clustering algorithm will then return a 1D numpy array of cluster labels. We will be using k-means, and before we use k-means, we use the silhouette score to determine the best k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the agglormerative clustering approach\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# building and fitting the clustering model\n",
    "agglomerative_model = AgglomerativeClustering(n_clusters=2)\n",
    "agglomerative_model.fit(top2)\n",
    "\n",
    "# retrieving the cluster labels\n",
    "agglomerative_labels = agglomerative_model.labels_\n",
    "\n",
    "# printing out the labels\n",
    "print(agglomerative_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
