{"category": "ham", "to_address": "Simon Blomberg <s.blomberg1@uq.edu.au>", "from_address": "paulandpen@optusnet.com.au", "subject": "Re: [R] Reduced Error Logistic Regression, and R?", "body": "Further to Simon's points,\n\nHere is what is confusing to me and I highlight the section of the claims below:\n\nThe key assumption concerns \"symmetrical error constraints\". These \"symmetrical error constraints\" force a solution where the probabilities of positive and negative error are symmetrical across all cross product sums that are the basis of maximum likelihood logistic regression. As the number of independent variables increases, it becomes more and more likely that this symmetrical assumption is accurate. Because this error component can be reliably estimated and subtracted out with a large enough number of variables, the resulting model parameters are strikingly error-free and do not overfit the data. \n\nFor me, maybe this is a bit old school here, but isn't the point of model development generating the most parsimonious model with the greatest explanatory power from the fewest variables.  I myself could just imagine going to a client and standing in a 'bored' (grin) room for a presentation, and saying hay client, here are the 200 variables that are driving choice behaviour.  I use latent class and bayes based approaches because they recover heterogeneity in utility allocation across the sample, that to me is a big battle in choice based analytics.  \n\nI believe that after a certain point, a heap of predictors become meaningless.  I can see some of my colleagues adopting this because it is in SAS and makes up for poor design.  \n\nAnyway, from a technical point of view, I would have to read a little about the error they are referring to.  Good on them for developing a new technology, like any algorithm, it will have its strengths and weaknesses and depending on factors such as usability etc, will gain some level of acceptance.     \n\nPaul\n\n\n> Simon Blomberg  wrote:\n> \n> >From what I've read (which isn't much), the idea is to estimate a\n> utility (preference) function for discrete categories, using logistic\n> regression, under the assumption that the residuals of the linear\n> predictor of the utilities are ~ Type I Gumbel. This implies the\n> \"independence of irrelevant alternatives\" in economic jargon. ie the\n> utility of choice a versus choice b is independent of the introduction\n> of a third choice c. It also implies homoscedasticity of the errors. The\n> model can be generalized in various ways. If you are willing to\n> introduce extra parameters into the model, such as the parameters of the\n> Gumbel distribution, you may get more precision in the estimates of the\n> utility function. An alternative (without the independence of irrelevant\n> alternatives assumption) is to model the errors as multivariate normal\n> (ie use probit regression), which is computationally much more\n> difficult.\n> \n> Whether it makes substantive sense to use these models outside of\n> \"discrete choice\" experiments is another question.\n> \n>  Patenting these methods is worrying. There have been a lot of people\n> working on discrete choice experiments over the years. It's hard to\n> believe that a single company could have ownership over an idea that is\n> the result of a collaborative effort such as this.\n> \n> Cheers,\n> \n> Simon.\n> \n>  On Thu, 2007-04-26 at 12:29 +1000, Tim Churches wrote:\n> > This news item in a data mining newsletter makes various claims for a \n> technique called \"Reduced Error Logistic Regression\": \n> http://www.kdnuggets.com/news/2007/n08/12i.html\n> > \n> > In brief, are these (ambitious) claims justified and if so, has this \n> technique been implemented in R (or does anyone have any plans to do \n> so)? \n> > \n> > Tim C\n> > \n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide \n> http://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> -- \n> Simon Blomberg, BSc (Hons), PhD, MAppStat. \n> Lecturer and Consultant Statistician \n> Faculty of Biological and Chemical Sciences \n> The University of Queensland \n> St. Lucia Queensland 4072 \n> Australia\n> \n> Room 320, Goddard Building (8)\n> T: +61 7 3365 2506 \n> email: S.Blomberg1_at_uq.edu.au \n> \n> The combination of some data and an aching desire for \n> an answer does not ensure that a reasonable answer can \n> be extracted from a given body of data. - John Tukey.\n> \n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide \n> http://www.R-project.org/posting-guide.html\n> and provide commented, minimal, self-contained, reproducible code.\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}