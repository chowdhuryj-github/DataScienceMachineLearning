{"category": "ham", "to_address": "\"jim holtman\" <jholtman@gmail.com>", "from_address": "\"Matthew Trunnell\" <trunnell@cognix.net>", "subject": "Re: [R] Histograms with strings, grouped by repeat count (w/ data)", "body": "Jim,\nThanks for the quick reply!  When I run your code, I end up with a\nsingle barplot of one datapoint, file9 vs email20 == 2.0.  I see the\ncall to barplot is inside a for loop... maybe it's zooming through the\ndisplay of many barplots, but all I see is the last one?\n\nIn any case, I need to figure out the distribution of the retries, such as\nNo. Retries   Count\n1                 6\n2                 13\n3                 5\n4                 3\n5                 2\n6                 1\n\nThat is, 6 people retried the download once; 13 people retried the\ndownload twice, etc.  So it would be counting the frequency of the\nemail-filename combination, and grouping those together by the number\nof retries.  Does that make sense?\n\nWhen I look at the counts object from your code, I can see that it's\nclose to what I need.  How do I access the properties of the counts\nobject-- it's a table, right?  If I look at counts[1,1], that returns\n1.  But how do I get at the row/col name of that cell?  Is that cell\nan object?  rownames(counts[1,1]) returns null.\n\nThanks,\nMatt\n\n\nOn 6/18/07, jim holtman  wrote:\n> You should be using barplot and not hist.  I think this produces what you\n> want:\n>\n> x <- \"filename,last_modified,email_addr,country_residence\n>\n> file1,3/4/2006 13:54,email1,Korea (South)\n> file2,3/4/2006 14:33,email2,United States\n> file2,3/4/2006 16:03,email2,United States\n> file2,3/4/2006 16:17,email3,United States\n> file2,3/4/2006 16:28,email3,United States\n> file3,3/4/2006 19:13,email4,United States\n> file2,3/4/2006 21:22,email5,India\n> file4,3/4/2006 21:46,email6,United States\n> file1,3/4/2006 22:04,email7,Japan\n> file2,3/4/2006 22:09,email8,Croatia\n> file1,3/4/2006 22:22,email7,Japan\n> file1,3/4/2006 22:29,email9,India\n> file1,3/4/2006 23:06,email6,United States\n> file1,3/4/2006 23:33,email6,United States\n> file5,3/4/2006 23:44,email10,China\n> file1,3/5/2006 0:13,email9,India\n> file2,3/5/2006 0:52,email8,Croatia\n> file2,3/5/2006 0:54,email8,Croatia\n> file2,3/5/2006 1:10,email5,India\n> file6,3/5/2006 2:17,email9,India\n> file2,3/5/2006 2:24,email11,Italy\n> file7,3/5/2006 2:36,email12,Italy\n> file8,3/5/2006 2:52,email12,Italy\n> file2,3/5/2006 3:09,email13,United Kingdom\n> file2,3/5/2006 4:02,email14,India\n> file2,3/5/2006 4:07,email14,India\n> file2,3/5/2006 4:14,email14,India\n> file2,3/5/2006 4:37,email5,India\n> file2,3/5/2006 4:44,email15,Belgium\n> file1,3/5/2006 5:02,email9,India\n> file1,3/5/2006 5:24,email16,Taiwan\n> file2,3/5/2006 6:06,email17,Saudi Arabia\n> file2,3/5/2006 7:32,email17,Saudi Arabia\n> file2,3/5/2006 8:12,email18,Brazil\n> file2,3/5/2006 8:26,email18,Brazil\n> file2,3/5/2006 9:49,email19,United Kingdom\n> file1,3/5/2006 10:49,email11,Italy\n> file1,3/5/2006 11:16,email13,United Kingdom\n> file1,3/5/2006 11:16,email13,United Kingdom\n> file1,3/5/2006 11:45,email13,United Kingdom\n> file1,3/5/2006 14:34,email20,Australia\n> file9,3/5/2006 14:56,email20,Australia\n> file9,3/5/2006 14:56,email20,Australia\n> file5,3/5/2006 16:43,email21,United States\n> file1,3/5/2006 17:17,email7,Japan\n> file2,3/5/2006 17:26,email22,Japan\n> file2,3/5/2006 17:27,email22,Japan\n> file2,3/5/2006 17:33,email23,China\n> file1,3/5/2006 17:45,email22,Japan\n> file2,3/5/2006 17:45,email22,Japan\n> file2,3/5/2006 17:59,email23,China\n> file1,3/5/2006 18:27,email24,Japan\n> file1,3/5/2006 18:47,email25,Taiwan\n> file2,3/5/2006 18:48,email26,New Zealand\n> file2,3/5/2006 19:15,email27,Canada\n> file2,3/5/2006 19:23,email28,Canada\n> file2,3/5/2006 19:24,email28,Canada\n> file10,3/5/2006 19:49,email29,Japan\n> file10,3/5/2006 19:52,email29,Japan\n> file10,3/5/2006 19:57,email29,Japan\n> file2,3/5/2006 20:01,email29,Japan\n> file2,3/5/2006 20:02,email29,Japan\n> file2,3/5/2006 20:06,email29,Japan\"\n> d <- read.csv(textConnection(x))\n> barplot(table(d$filename), main=\"All Files\", las=2)  # plot counts for all\n> the files\n> # generate plots for each file name showing which emails used them\n> counts <- table(d$filename, d$email_addr)\n> for (i in seq(nrow(counts))){\n>     .index <- which(counts[i,] > 0)\n>     barplot(counts[i, .index], las=2,\n>         names.arg=colnames(counts)[.index], main=rownames(counts)[i])\n> }\n>\n>\n>\n> On 6/18/07, Matthew Trunnell  wrote:\n> >\n> > Hello R gurus,\n> >\n> > I just spent my first weekend wrestling with R, but so far have come\n> > up empty handed.\n> >\n> > I have a dataset that represents file downloads; it has 4 dimensions:\n> > date, filename, email, and country.  (sample data below)\n> >\n> > My first goal is to get an idea of the frequency of repeated\n> > downloads.  Let me explain that.  Some people tend to download\n> > multiple times, e.g. if the download fails they keep trying over and\n> > over.  I'm trying to build a histogram that shows the repeat count\n> > along the x-axis, that is, how many people downloaded once, twice,\n> > three times, etc.  I plan to compare the median of that before and\n> > after we switched ISPs.\n> >\n> > To accomplish this, I'm assuming that I'll first need to combine the\n> > email and filename columns so as to represent a single download\n> > attempt by an individual.  Does that sound right?  Later, it would be\n> > nice to limit the histogram to a single filename, country, or company.\n> > I can probably figure that out myself after I understand how to write\n> > this funky histogram expression.\n> >\n> > With the help of Verzani's introductory text, I've learned how to read\n> > in the CSV data and do some simple tables, like this:\n> >\n> > hist(table(d$filename))\n> > hist(table(d$filename[substring(d$filename, 1,\n> 5)==\"file1\"]))\n> > hist(sort(table(d$filename[substring(d$filename, 1,\n> 5)==\"file1\"])))\n> >\n> > Obviously, these commands count the frequency of the files.  What I'd\n> > like to see are the repeats grouped along the x-axis;  I'd like to\n> > find, for all files, the distribution of retries.  I hope that makes\n> > sense. :)\n> >\n> > Can someone point me in the right direction?  I'm very new to R and to\n> > statistics, but I write code for a living.  At this point I'd almost\n> > be better off writing a program do this kind of simple counting... but\n> > I have a feeling R would be so useful if I could just get past the\n> > initial learning curve.\n> >\n> > Thank you in advance,\n> > Matt\n> >\n> > Here's some real data, with the private info replaced :)\n> >\n> > d<-read.table\n> (file=\"C:\\\\users\\\\trunnellm\\\\downloads\\\\statistics\\\\downloads.csv\",\n> > sep=\",\", quote=\"\\\"\", header=TRUE)\n> >\n> > filename,last_modified,email_addr,country_residence\n> > file1,3/4/2006 13:54,email1,Korea (South)\n> > file2,3/4/2006 14:33,email2,United States\n> > file2,3/4/2006 16:03,email2,United States\n> > file2,3/4/2006 16:17,email3,United States\n> > file2,3/4/2006 16:28,email3,United States\n> > file3,3/4/2006 19:13,email4,United States\n> > file2,3/4/2006 21:22,email5,India\n> > file4,3/4/2006 21:46,email6,United States\n> > file1,3/4/2006 22:04,email7,Japan\n> > file2,3/4/2006 22:09,email8,Croatia\n> > file1,3/4/2006 22:22,email7,Japan\n> > file1,3/4/2006 22:29,email9,India\n> > file1,3/4/2006 23:06,email6,United States\n> > file1,3/4/2006 23:33,email6,United States\n> > file5,3/4/2006 23:44,email10,China\n> > file1,3/5/2006 0:13,email9,India\n> > file2,3/5/2006 0:52,email8,Croatia\n> > file2,3/5/2006 0:54,email8,Croatia\n> > file2,3/5/2006 1:10,email5,India\n> > file6,3/5/2006 2:17,email9,India\n> > file2,3/5/2006 2:24,email11,Italy\n> > file7,3/5/2006 2:36,email12,Italy\n> > file8,3/5/2006 2:52,email12,Italy\n> > file2,3/5/2006 3:09,email13,United Kingdom\n> > file2,3/5/2006 4:02,email14,India\n> > file2,3/5/2006 4:07,email14,India\n> > file2,3/5/2006 4:14,email14,India\n> > file2,3/5/2006 4:37,email5,India\n> > file2,3/5/2006 4:44,email15,Belgium\n> > file1,3/5/2006 5:02,email9,India\n> > file1,3/5/2006 5:24,email16,Taiwan\n> > file2,3/5/2006 6:06,email17,Saudi Arabia\n> > file2,3/5/2006 7:32,email17,Saudi Arabia\n> > file2,3/5/2006 8:12,email18,Brazil\n> > file2,3/5/2006 8:26,email18,Brazil\n> > file2,3/5/2006 9:49,email19,United Kingdom\n> > file1,3/5/2006 10:49,email11,Italy\n> > file1,3/5/2006 11:16,email13,United Kingdom\n> > file1,3/5/2006 11:16,email13,United Kingdom\n> > file1,3/5/2006 11:45,email13,United Kingdom\n> > file1,3/5/2006 14:34,email20,Australia\n> > file9,3/5/2006 14:56,email20,Australia\n> > file9,3/5/2006 14:56,email20,Australia\n> > file5,3/5/2006 16:43,email21,United States\n> > file1,3/5/2006 17:17,email7,Japan\n> > file2,3/5/2006 17:26,email22,Japan\n> > file2,3/5/2006 17:27,email22,Japan\n> > file2,3/5/2006 17:33,email23,China\n> > file1,3/5/2006 17:45,email22,Japan\n> > file2,3/5/2006 17:45,email22,Japan\n> > file2,3/5/2006 17:59,email23,China\n> > file1,3/5/2006 18:27,email24,Japan\n> > file1,3/5/2006 18:47,email25,Taiwan\n> > file2,3/5/2006 18:48,email26,New Zealand\n> > file2,3/5/2006 19:15,email27,Canada\n> > file2,3/5/2006 19:23,email28,Canada\n> > file2,3/5/2006 19:24,email28,Canada\n> > file10,3/5/2006 19:49,email29,Japan\n> > file10,3/5/2006 19:52,email29,Japan\n> > file10,3/5/2006 19:57,email29,Japan\n> > file2,3/5/2006 20:01,email29,Japan\n> > file2,3/5/2006 20:02,email29,Japan\n> > file2,3/5/2006 20:06,email29,Japan\n> >\n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide\n> http://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> >\n>\n>\n>\n> --\n> Jim Holtman\n> Cincinnati, OH\n> +1 513 646 9390\n>\n> What is the problem you are trying to solve?\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}