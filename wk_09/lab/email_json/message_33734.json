{"category": "ham", "to_address": "\"=?GB2312?B?wO6/ob3c?=\" <klijunjie@gmail.com>", "from_address": "\"Paul Lynch\" <plynchnlm@gmail.com>", "subject": "Re: [R] R2 always increases as variables are added?", "body": "Junjie,\n    First, a disclaimer:  I am not a statistician, and have only taken\none statistics class, but I just took it this Spring, so the concepts\nof linear regression are relatively fresh in my head and hopefully I\nwill not be too inaccurate.\n    According to my statistics textbook, when selecting variables for\na model, the intercept term is always present.  The \"variables\" under\nconsideration do not include the constant \"1\" that multiplies the\nintercept term.  I don't think it makes sense to compare models with\nand without an intercept term.  (Also, I don't know what the point of\nusing a model without an intercept term would be, but that is probably\njust my ignorance.)\n    Similarly, the formula you were using for R**2 seems to only be\nuseful in the context of a standard linear regression (i.e., one that\nincludes an intercept term).  As your example shows, it is easy to\nconstruct a \"fit\" (e.g. y = 10,000,000*x) so that SSR > SST if one is\nnot deriving the fit from the regular linear regression process.\n          --Paul\n\nOn 5/19/07, \u001b$BM{=S[?\u001b(B  wrote:\n> I know that \"-1\" indicates to remove the intercept term. But my question is\n> why intercept term CAN NOT be treated as a variable term as we place a\n> column consited of 1 in the predictor matrix.\n>\n> If I stick to make a comparison between a model with intercept and one\n> without intercept on adjusted r2 term, now I think the strategy is always to\n> use another definition of r-square or adjusted r-square, in which\n> r-square=sum(( y.hat)^2)/sum((y)^2).\n>\n> Am I  in the right way?\n>\n> Thanks\n>\n> Li Junjie\n>\n>\n> 2007/5/19, Paul Lynch :\n> > In case you weren't aware, the meaning of the \"-1\" in y ~ x - 1 is to\n> > remove the intercept term that would otherwise be implied.\n> >     --Paul\n> >\n> > On 5/17/07, \u001b$BM{=S[?\u001b(B  wrote:\n> > > Hi, everybody,\n> > >\n> > > 3 questions about R-square:\n> > > ---------(1)----------- Does R2 always increase as variables are added?\n> > > ---------(2)----------- Does R2 always greater than 1?\n> > > ---------(3)----------- How is R2 in summary(lm(y~x-1))$r.squared\n> > > calculated? It is different from (r.square=sum((y.hat-mean\n> > > (y))^2)/sum((y-mean(y))^2))\n> > >\n> > > I will illustrate these problems by the following codes:\n> > > ---------(1)-----------  R2  doesn't always increase as\n> variables are added\n> > >\n> > > > x=matrix(rnorm(20),ncol=2)\n> > > > y=rnorm(10)\n> > > >\n> > > > lm=lm(y~1)\n> > > > y.hat=rep(1*lm$coefficients,length(y))\n> > > > (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> > > [1] 2.646815e-33\n> > > >\n> > > > lm=lm(y~x-1)\n> > > > y.hat=x%*%lm$coefficients\n> > > > (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> > > [1] 0.4443356\n> > > >\n> > > > ################ This is the biggest model, but its R2 is not the\n> biggest,\n> > > why?\n> > > > lm=lm(y~x)\n> > > > y.hat=cbind(rep(1,length(y)),x)%*%lm$coefficients\n> > > > (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> > > [1] 0.2704789\n> > >\n> > >\n> > > ---------(2)-----------  R2  can greater than 1\n> > >\n> > > > x=rnorm(10)\n> > > > y=runif(10)\n> > > > lm=lm(y~x-1)\n> > > > y.hat=x*lm$coefficients\n> > > > (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> > > [1] 3.513865\n> > >\n> > >\n> > >  ---------(3)----------- How is R2 in summary(lm(y~x-1))$r.squared\n> > > calculated? It is different from (r.square=sum((y.hat-mean\n> > > (y))^2)/sum((y-mean(y))^2))\n> > > > x=matrix(rnorm(20),ncol=2)\n> > > > xx=cbind(rep(1,10),x)\n> > > > y=x%*%c(1,2)+rnorm(10)\n> > > > ### r2 calculated by lm(y~x)\n> > > > lm=lm(y~x)\n> > > > summary(lm)$r.squared\n> > > [1] 0.9231062\n> > > > ### r2 calculated by lm(y~xx-1)\n> > > > lm=lm(y~xx-1)\n> > > > summary(lm)$r.squared\n> > > [1] 0.9365253\n> > > > ### r2 calculated by me\n> > > > y.hat=xx%*%lm$coefficients\n> > > > (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> > > [1] 0.9231062\n> > >\n> > >\n> > > Thanks a lot for any cue:)\n> > >\n> > >\n> > >\n> > >\n> > > --\n> > > Junjie Li,                  klijunjie@gmail.com\n> > > Undergranduate in DEP of Tsinghua University,\n> > >\n> > >         [[alternative HTML version deleted]]\n> > >\n> > > ______________________________________________\n> > > R-help@stat.math.ethz.ch mailing list\n> > > https://stat.ethz.ch/mailman/listinfo/r-help\n> > > PLEASE do read the posting guide\n> http://www.R-project.org/posting-guide.html\n> > > and provide commented, minimal, self-contained, reproducible code.\n> > >\n> >\n> >\n> > --\n> > Paul Lynch\n> > Aquilent, Inc.\n> > National Library of Medicine (Contractor)\n> >\n>\n>\n>\n> --\n>\n> Junjie Li,                  klijunjie@gmail.com\n> Undergranduate in DEP of Tsinghua University,\n\n\n-- \nPaul Lynch\nAquilent, Inc.\nNational Library of Medicine (Contractor)\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}