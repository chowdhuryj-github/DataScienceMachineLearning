{"category": "ham", "to_address": "Victor Gravenholt <victor.gravenholt@gmail.com>", "from_address": "Prof Brian Ripley <ripley@stats.ox.ac.uk>", "subject": "Re: [R] sample function and memory usage", "body": "On Tue, 8 May 2007, Victor Gravenholt wrote:\n\n> As a part of a simulation, I need to sample from a large vector repeatedly.\n> For some reason sample() builds up the memory usage (> 500 MB for this\n> example) when used inside a for loop as illustrated here:\n>\n> X <- 1:100000\n> P <- runif(100000)\n> for(i in 1:500) Xsamp <- sample(X,30000,replace=TRUE,prob=P)\n>\n> Even worse, I am not able to free up memory without quitting R.\n> I quickly run out of memory when trying to perform the simulation. Is\n> there any way to avoid this to happen?\n>\n> The problem seem to appear only when specifying both replace=TRUE and\n> probability weights for the vector being sampled, and this happens both\n> on Windows XP and Linux (Ubuntu).\n\nAnd for 10000 < size <= 100000.  There was a typo causing memory not to be \nfreed in that range.  It is now fixed in 2.5.0 patched.\n\n-- \nBrian D. Ripley,                  ripley@stats.ox.ac.uk\nProfessor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/\nUniversity of Oxford,             Tel:  +44 1865 272861 (self)\n1 South Parks Road,                     +44 1865 272866 (PA)\nOxford OX1 3TG, UK                Fax:  +44 1865 272595\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}