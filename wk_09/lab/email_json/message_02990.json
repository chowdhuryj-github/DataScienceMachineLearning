{"category": "ham", "to_address": "\"Robert Duval\" <rduval@gmail.com>", "from_address": "\"Spielas Bates\" <bates@stat.wisc.edu>", "subject": "Re: [R] Reasons to Use R", "body": "On 4/11/07, Robert Duval  wrote:\n> So I guess my question is...\n>\n> Is there any hope of R being modified on its core in order to handle\n> more graciously large datasets? (You've mentioned SAS and SPSS, I'd\n> add Stata to the list).\n>\n> Or should we (the users of large datasets) expect to keep on working\n> with the present tools for the time to come?\n\nWe're certainly aware of the desire of many users to be able to handle\nlarge data sets.  I have just spent a couple of days working with a\nstudent from another department who wanted to work with a very large\ndata set that was poorly structured.  Most of my time was spent trying\nto convince her about the limitations in the structure of her data and\nwhat could realistically be expected to be computed with it.\n\nIf your purpose is to perform data manipulation and extraction on\nlarge data sets then I think that it is not unreasonable to be\nexpected to learn to use SQL. I find it convenient to use R to do data\nmanipulation because I know the language and the support tools well\nbut I don't expect to do data cleaning on millions of records with it.\n I am probably too conservative in what I will ask R to handle for me\nbecause I started using S on a Vax-11/750 that had 2 megabytes of\nmemory and it's hard to break old habits.\n\nI think the trend in working with large data sets in R will be toward\na hybrid approach of using a database for data storage and retrieval\nplus R for the model definition and computation.  Miguel Manese's\nSQLiteDF package and some of the work in Bioconductor are steps in\nthis direction.\n\nHowever, as was mentioned earlier in this thread, there is an\nunderlying assumption with R that the user is thinking about the\nanalysis as he/she is doing it. We sometimes see questions about \"I\nhave a data set with (some large number) of records on several hundred\nor thousands of variables\" and I want to fit a generalized linear\nmodel to it.\n\nI would be hard pressed to think of a situation where I wanted\nhundreds of variables in a statistical model unless they are generated\nfrom one or more factors that have many levels.  And, in that case, I\nwould want to use random effects rather than fixed effects in a model.\n So just saying that the big challenge is to fit some kind of model\nwith lots of coefficients to a very large number of observations may\nbe missing the point.  Defining the model better may be the point.\n\nLet me conclude by saying that these are general observations and not\ndirected to you personally, Robert.  I don't know what you want R to\ndo graciously to large data sets so my response is more to the general\npoint that there should always be a balance between thinking about the\nstructure of the data and the model and brute force computation.  One\ncan do data analysis by using the computer as a blunt instrument with\nwhich to bludgeon the problem to death but one can't do elegant data\nanalysis like that.\n\n\n\n\n>\n> robert\n>\n> On 4/11/07, Marc Schwartz  wrote:\n> > On Wed, 2007-04-11 at 11:26 -0500, Marc Schwartz wrote:\n> > > On Wed, 2007-04-11 at 17:56 +0200, Bi-Info\n> > > (http://members.home.nl/bi-info) wrote:\n> > > > I certainly have that idea too. SPSS functions in a way the same,\n> > > > although it specialises in PC applications. Memory addition to a PC is\n> > > > not a very expensive thing these days. On my first AT some extra memory\n> > > > cost 300 dollars or more. These days you get extra memory with a package\n> > > > of marshmellows or chocolate bars if you need it.\n> > > > All computations on a computer are discrete steps in a way, but I've\n> > > > heard that SAS computations are split up in strictly divided steps. That\n> > > > also makes procedures \"attachable\" I've been told, and interchangable.\n> > > > Different procedures can use the same code which alternatively is\n> > > > cheaper in memory usages or disk usage (the old days...). That makes SAS\n> > > > by the way a complicated machine to build because procedures who are\n> > > > split up into numerous fragments which make complicated bookkeeping. If\n> > > > you do it that way, I've been told, you can do a lot of computations\n> > > > with very little memory. One guy actually computed quite complicated\n> > > > models with \"only 32MB or less\", which wasn't very much for \"his type of\n> > > > calculations\". Which means that SAS is efficient in memory handling I\n> > > > think. It's not very efficient in dollar handling... I estimate.\n> > > >\n> > > > Wilfred\n> > >\n> > > \n> > >\n> > > Oh....SAS is quite efficient in dollar handling, at least when it comes\n> > > to the annual commercial licenses...along the same lines as the\n> > > purported efficiency of the U.S. income tax system:\n> > >\n> > >   \"How much money do you have?  Send it in...\"\n> > >\n> > > There is a reason why SAS is the largest privately held software company\n> > > in the world and it is not due to the academic licensing structure,\n> > > which constitutes only about 12% of their revenue, based upon their\n> > > public figures.\n> >\n> > Hmmm......here is a classic example of the problems of reading pie\n> > charts.\n> >\n> > The figure I quoted above, which is from reading the 2005 SAS Annual\n> > Report on their web site (such as it is for a private company) comes\n> > from a 3D exploded pie chart (ick...).\n> >\n> > The pie chart uses 3 shades of grey and 5 shades of blue to\n> > differentiate 8 market segments and their percentages of total worldwide\n> > revenue.\n> >\n> > I mis-read the 'shade of grey' allocated to Education as being 12%\n> > (actually 11.7%).\n> >\n> > A re-read of the chart, zooming in close on the pie in a PDF reader,\n> > appears to actually show that Education is but 1.8% of their annual\n> > worldwide revenue.\n> >\n> > Government based installations, which are presumably the other notable\n> > market segment in which substantially discounted licenses are provided,\n> > is 14.6%.\n> >\n> > The report is available here for anyone else curious:\n> >\n> >   http://www.sas.com/corporate/report05/annualreport05.pdf\n> >\n> > Somebody needs to send SAS a copy of Tufte or Cleveland.\n> >\n> > I have to go and rest my eyes now...  ;-)\n> >\n> > Regards,\n> >\n> > Marc\n> >\n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> >\n>\n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html\n> and provide commented, minimal, self-contained, reproducible code.\n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}