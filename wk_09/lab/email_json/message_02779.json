{"category": "ham", "to_address": "\"Spielas Bates\" <bates@stat.wisc.edu>, \"Wensui Liu\" <liuwensui@gmail.com>", "from_address": "\"Liaw, Andy\" <andy_liaw@merck.com>", "subject": "Re: [R] Reasons to Use R  [Broadcast]", "body": "From: Spielas Bates\n> \n> On 4/10/07, Wensui Liu  wrote:\n> > Greg,\n> > As far as I understand, SAS is more efficient handling large data \n> > probably than S+/R. Do you have any idea why?\n> \n> SAS originated at a time when large data sets were stored on \n> magnetic tape and the only reasonable way to process them was \n> sequentially.\n> Thus most statistics procedures in SAS act as filters, \n> processing one record at a time and accumulating summary \n> information.  In the past SAS performed a least squares fit \n> by accumulating the crossproduct of [X:y] and then using the \n> using the sweep operator to reduce that matrix. For such an \n> approach the number of observations does not affect the \n> amount of storage required.  Adding observations just \n> requires more time.\n> \n> This works fine (although there are numerical disadvantages \n> to this approach - try mentioning the sweep operator to an \n> expert in numerical linear algebra - you get a blank stare) \n\nFor those who stared blankly at the above:  The sweep operator is \njust a facier version of the good old Gaussian elimination...\n\nAndy\n\n> as long as the operations that you wish to perform fit into \n> this model.  Making the desired operations fit into the model \n> is the primary reason for the awkwardness in many SAS analyses.\n> \n> The emphasis in R is on flexibility and the use of good \n> numerical techniques - not on processing large data sets \n> sequentially.  The algorithms used in R for most least \n> squares fits generate and analyze the complete model matrix \n> instead of summary quantities.  (The algorithms in the biglm \n> package are a compromise that work on horizontal sections of \n> the model matrix.)\n> \n> If your only criterion for comparison is the ability to work \n> with very large data sets performing operations that can fit \n> into the filter model used by SAS then SAS will be a better \n> choice.  However you do lock yourself into a certain set of \n> operations and you are doing it to save memory, which is a \n> commodity that decreases in price very rapidly.\n> \n> As mentioned in other replies, for many years the majority of \n> SAS uses are for data manipulation rather than for \n> statistical analysis so the filter model has been modified in \n> later versions.\n> \n> \n> \n> \n> \n> > On 4/10/07, Greg Snow  wrote:\n> > > > -----Original Message-----\n> > > > From: r-help-bounces@stat.math.ethz.ch \n> > > > [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of Bi-Info \n> > > > (http://members.home.nl/bi-info)\n> > > > Sent: Monday, April 09, 2007 4:23 PM\n> > > > To: Gabor Grothendieck\n> > > > Cc: Lorenzo Isella; r-help@stat.math.ethz.ch\n> > > > Subject: Re: [R] Reasons to Use R\n> > >\n> > > [snip]\n> > >\n> > > > So what's the big deal about S using files instead of \n> memory like \n> > > > R. I don't get the point. Isn't there enough swap space for S? \n> > > > (Who cares\n> > > > anyway: it works, isn't it?) Or are there any problems \n> with S and \n> > > > large datasets? I don't get it. You use them, Greg. So \n> you might \n> > > > discuss that issue.\n> > > >\n> > > > Wilfred\n> > > >\n> > > >\n> > >\n> > > This is my understanding of the issue (not anything official).\n> > >\n> > > If you use up all the memory while in R, then the OS will start \n> > > swapping memory to disk, but the OS does not know what parts of \n> > > memory correspond to which objects, so it is entirely \n> possible that \n> > > the chunk swapped to disk contains parts of different \n> data objects, \n> > > so when you need one of those objects again, everything \n> needs to be \n> > > swapped back in.  This is very inefficient.\n> > >\n> > > S-PLUS occasionally runs into the same problem, but since it does \n> > > some of its own swapping to disk it can be more efficient by \n> > > swapping single data objects (data frames, etc.).  Also, since \n> > > S-PLUS is already saving everything to disk, it does not actually \n> > > need to do a full swap, it can just look and see that a \n> particular \n> > > data frame has not been used for a while, know that it is already \n> > > saved on the disk, and unload it from memory without \n> having to write it to disk first.\n> > >\n> > > The g.data package for R has some of this functionality \n> of keeping \n> > > data on the disk until needed.\n> > >\n> > > The better approach for large data sets is to only have \n> some of the \n> > > data in memory at a time and to automatically read just the parts \n> > > that you need.  So for big datasets it is recommended to have the \n> > > actual data stored in a database and use one of the database \n> > > connection packages to only read in the subset that you \n> need.  The \n> > > SQLiteDF package for R is working on automating this \n> process for R.  \n> > > There are also the bigdata module for S-PLUS and the \n> biglm package \n> > > for R have ways of doing some of the common analyses \n> using chunks of \n> > > data at a time.  This idea is not new.  There was a \n> program in the \n> > > late 1970s and 80s called Rummage by Del Scott (I guess \n> technically it still exists, I have a copy on a 5.25\"\n> > > floppy somewhere) that used the approach of specify the model you \n> > > wanted to fit first, then specify the data file.  Rummage \n> would then \n> > > figure out which sufficient statistics were needed and \n> read the data \n> > > in chunks, compute the sufficient statistics on the fly, and not \n> > > keep more than a couple of lines of the data in memory at once.  \n> > > Unfortunately it did not have much of a user interface, so when \n> > > memory was cheap and datasets only medium sized it did \n> not compete \n> > > well, I guess it was just a bit too ahead of its time.\n> > >\n> > > Hope this helps,\n> > >\n> > >\n> > >\n> > > --\n> > > Gregory (Greg) L. Snow Ph.D.\n> > > Statistical Data Center\n> > > Intermountain Healthcare\n> > > greg.snow@intermountainmail.org\n> > > (801) 408-8111\n> > >\n> > > ______________________________________________\n> > > R-help@stat.math.ethz.ch mailing list \n> > > https://stat.ethz.ch/mailman/listinfo/r-help\n> > > PLEASE do read the posting guide \n> > > http://www.R-project.org/posting-guide.html\n> > > and provide commented, minimal, self-contained, reproducible code.\n> > >\n> >\n> >\n> > --\n> > WenSui Liu\n> > A lousy statistician who happens to know a little programming\n> > (http://spaces.msn.com/statcompute/blog)\n> >\n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide \n> > http://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> >\n> \n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide \n> http://www.R-project.org/posting-guide.html\n> and provide commented, minimal, self-contained, reproducible code.\n> \n> \n> \n\n\n------------------------------------------------------------------------------\nNotice:  This e-mail message, together with any attachments,...{{dropped}}\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}