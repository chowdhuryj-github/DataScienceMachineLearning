{"category": "ham", "to_address": "\"AJ Rossini\" <blindglobe@gmail.com>", "from_address": "\"Ramon Diaz-Uriarte\" <rdiaz02@gmail.com>", "subject": "Re: [R] Rserve and R to R communication", "body": "On 4/10/07, AJ Rossini  wrote:\n> On Monday 09 April 2007 23:02, Ramon Diaz-Uriarte wrote:\n>\n> > (Yes, maybe I should check snowFT, but it uses PVM, and I recall a\n> > while back there was a reason why we decided to go with MPI instead of\n> > PVM).\n>\n> There is no reason that you can't run both MPI and PVM on the same cluster.\n>\n\nYes, sure. We actually did that for a while. But we eventually settled on MPI.\n\n> There is a particular reason that the first implementation we (Na Li, who did\n> most of the work, and myself) made used PVM -- at the time (pre MPI 2) it was\n> far more advanced than MPI as far as \"interactive parallel computing\", i.e.\n> dispatch parallel functions interactively from the command line, creating and\n> manipulating virtual machines on the fly.\n>\n\nOf course, you are right there. I think that might still be the case.\nAt the time we made our decision, and decided to go for MPI, MPI 2 was\nalready out, and MPI seemed \"more like the current/future standard\"\nthan PVM. A feeling that was reinforced by seeing some key people of\nPVM (e.g., Dongarra) also involved in MPI, as well as very active\ndevelopment of MPI (e.g., LAM, mpich, and later OpenMPI). And MPI\nseemed more like \"the usual message passing\" (which for us was, at\nthat time at least, a good thing). And we were also using MPI in C++\ncode. So we decided to bet on MPI.\n\n\n> Of course, most MPI implementations will save you loads of deci-seconds on\n> transfer of medium size messages over the wire, but we weren't interested in\n\nOh, but those deci-seconds were never the reason we decided to choose\nMPI. We are using R after all, not HPF :-).\n\n> that particular aspect, more in saving days over the course of a one-off\n> program (i.e. development time, which can be more painful that run-time).\n>\n\nRight. And of course we never thought MPI would cost us significantly\nmore development time than PVM (and that the increased development\ntime would be compensated by the above mentioned deci-seconds).\nMoreover, most of these are not one-off programs, but web applications\n(some of which have been running for over two years) where easy\ndebugging is crucial for us if we have to revisit the code 6 months\nlater (and for that we found papply quite more useful than snow\n---more below).\n\n\n> Now, PVM had the necessary tools for fault tolerance -- though I thought that\n> the recent MPI and newer message passing frameworks might have had some of\n> that implemented.\n>\n\nSome MPIs have been developed that incorporate it. But I do not think\nthat is easy with LAM/MPI nor via Rmpi. The problem is that once a\nnode goes down, the whole LAM universe gets screwed up.\n\n> And remember, the point of snow was to provide platform-independent parallel\n> code (for which it was the first, for nearly any language/implementation),\n> not to run it like a bat-out-of-hell...  (we assumed it would be cheaper to\n> buy more machines than to spend a few months finding a budget along with\n> sharp programmers).\n>\n\nSo using papply with Rmpi requires sharper programmers than using\nsnow? Hey, it is good to know I am that smarter. I'll wear that as a\nbadge :-).\n\nAnyway, papply (with Rmpi) is not, in my experience, any harder than\nsnow (with either rpvm or Rmpi). In fact, I find papply a lot simpler\nthan snow (clusterApply and clusterApplyLB). For one thing, debugging\nis very simple, since papply becomes lapply if no lam universe is\nbooted.\n\n\nI see, though, that I might want to check PVM just for the sake of the\nfault tolerance in snowFT.\n\n\nBest,\n\nR.\n\n> best,\n> -tony\n>\n> blindglobe@gmail.com\n> Muttenz, Switzerland.\n> \"Commit early,commit often, and commit in a repository from which we can\n> easily\n> roll-back your mistakes\" (AJR, 4Jan05).\n>\n>\n\n\n-- \nRamon Diaz-Uriarte\nStatistical Computing Team\nStructural Biology and Biocomputing Programme\nSpanish National Cancer Centre (CNIO)\nhttp://ligarto.org/rdiaz\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}