{"category": "ham", "to_address": "R-help <r-help@stat.math.ethz.ch>", "from_address": "\"Paul Smith\" <phhs80@gmail.com>", "subject": "Re: [R] Bad optimization solution", "body": "Thanks, Ravi, for your clear explanation!\n\nPaul\n\n\nOn 5/8/07, RAVI VARADHAN  wrote:\n> Paul,\n>\n> The problem lies neither with R nor with numercial methods.  The onus is always on the user to understand what the numerical schemes can do and what they can't do.  One should never blindly take the results given by a numerical scheme and run with it.  In your example, the optimization method is doing what it was designed to do: to find a critical point of a function where the gradient is zero.  It is your responsibility to ensure that the result makes sense, and if it doesn't, to understand why it doesn't make sense.  In your problem, maxima ((1,0) and (0,1)) lie on the boundary of the parameter space, and the gradient at the maxima (defined as the limit from within the domain) are clearly not zero.  Another problem with your example is that the hessian for your function is singular, it has eigenvalues of 0 and 2.  In short, there is no substitute to using your analytic powers!\n>\n> Ravi.\n>\n> ----- Original Message -----\n> From: Paul Smith \n> Date: Tuesday, May 8, 2007 4:33 am\n> Subject: Re: [R] Bad optimization solution\n> To: R-help \n>\n>\n> > It seems that there is here a problem of reliability, as one never\n> >  knows whether the solution provided by R is correct or not. In the\n> >  case that I reported, it is fairly simple to see that the solution\n> >  provided by R (without any warning!) is incorrect, but, in general,\n> >  that is not so simple and one may take a wrong solution as a correct\n> >  one.\n> >\n> >  Paul\n> >\n> >\n> >  On 5/8/07, Ravi Varadhan  wrote:\n> >  > Your function, (x1-x2)^2, has zero gradient at all the starting\n> > values such\n> >  > that x1 = x2, which means that the gradient-based search methods will\n> >  > terminate there because they have found a critical point, i.e. a\n> > point at\n> >  > which the gradient is zero (which can be a maximum or a minimum or\n> > a saddle\n> >  > point).\n> >  >\n> >  > However, I do not why optim converges to the boundary maximum, when\n> > analytic\n> >  > gradient is supplied (as shown by Sundar).\n> >  >\n> >  > Ravi.\n> >  >\n> >  > ----------------------------------------------------------------------------\n> >  > -------\n> >  >\n> >  > Ravi Varadhan, Ph.D.\n> >  >\n> >  > Assistant Professor, The Center on Aging and Health\n> >  >\n> >  > Division of Geriatric Medicine and Gerontology\n> >  >\n> >  > Johns Hopkins University\n> >  >\n> >  > Ph: (410) 502-2619\n> >  >\n> >  > Fax: (410) 614-9625\n> >  >\n> >  > Email: rvaradhan@jhmi.edu\n> >  >\n> >  > Webpage:\n> >  >\n> >  >\n> >  >\n> >  > ----------------------------------------------------------------------------\n> >  > --------\n> >  >\n> >  >\n> >  > -----Original Message-----\n> >  > From: r-help-bounces@stat.math.ethz.ch\n> >  > [ On Behalf Of Paul Smith\n> >  > Sent: Monday, May 07, 2007 6:26 PM\n> >  > To: R-help\n> >  > Subject: Re: [R] Bad optimization solution\n> >  >\n> >  > On 5/7/07, Paul Smith  wrote:\n> >  > > > I think the problem is the starting point.  I do not remember the\n> >  > details\n> >  > > > of the BFGS method, but I am almost sure the (.5, .5) starting\n> > point is\n> >  > > > suspect, since the abs function is not differentiable at 0.  If\n> > you\n> >  > perturb\n> >  > > > the starting point even slightly you will have no problem.\n> >  > > >\n> >  > > >              \"Paul Smith\"\n> >  > > >               >  > > >              >\n> >  > To\n> >  > > >              Sent by:                  R-help \n> >  > > >              r-help-bounces@st\n> >  > cc\n> >  > > >              at.math.ethz.ch\n> >  > > >\n> >  > Subject\n> >  > > >                                        [R] Bad optimization solution\n> >  > > >              05/07/2007 04:30\n> >  > > >              PM\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > >\n> >  > > > Dear All\n> >  > > >\n> >  > > > I am trying to perform the below optimization problem, but getting\n> >  > > > (0.5,0.5) as optimal solution, which is wrong; the correct solution\n> >  > > > should be (1,0) or (0,1).\n> >  > > >\n> >  > > > Am I doing something wrong? I am using R 2.5.0 on Fedora Core 6\n> > (Linux).\n> >  > > >\n> >  > > > Thanks in advance,\n> >  > > >\n> >  > > > Paul\n> >  > > >\n> >  > > > ------------------------------------------------------\n> >  > > > myfunc <- function(x) {\n> >  > > >   x1 <- x[1]\n> >  > > >   x2 <- x[2]\n> >  > > >   abs(x1-x2)\n> >  > > > }\n> >  > > >\n> >  > > >\n> >  > optim(c(0.5,0.5),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=\n> >  > list(fnscale=-1))\n> >  > >\n> >  > > Yes, with (0.2,0.9), a correct solution comes out. However, how can\n> >  > > one be sure in general that the solution obtained by optim is correct?\n> >  > > In ?optim says:\n> >  > >\n> >  > >      Method '\"L-BFGS-B\"' is that of Byrd _et. al._ (1995) which allows\n> >  > >      _box constraints_, that is each variable can be given a lower\n> >  > >      and/or upper bound. The initial value must satisfy the\n> >  > >      constraints. This uses a limited-memory modification of the\n> > BFGS\n> >  > >      quasi-Newton method. If non-trivial bounds are supplied, this\n> >  > >      method will be selected, with a warning.\n> >  > >\n> >  > > which only demands that \"the initial value must satisfy the constraints\".\n> >  >\n> >  > Furthermore, X^2 is everywhere differentiable and notwithstanding the\n> >  > reported problem occurs with\n> >  >\n> >  > myfunc <- function(x) {\n> >  >   x1 <- x[1]\n> >  >   x2 <- x[2]\n> >  >   (x1-x2)^2\n> >  > }\n> >  >\n> >  > optim(c(0.2,0.2),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=\n> >  > list(fnscale=-1))\n> >  >\n> >  > Paul\n> >  >\n> >  > ______________________________________________\n> >  > R-help@stat.math.ethz.ch mailing list\n> >  >\n> >  > PLEASE do read the posting guide\n> >  > and provide commented, minimal, self-contained, reproducible code.\n> >  >\n> >\n> >  ______________________________________________\n> >  R-help@stat.math.ethz.ch mailing list\n> >\n> >  PLEASE do read the posting guide\n> >  and provide commented, minimal, self-contained, reproducible code.\n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}