{"category": "ham", "to_address": "r-help@stat.math.ethz.ch", "from_address": "(Ted Harding) <ted.harding@nessie.mcc.ac.uk>", "subject": "Re: [R] Tools For Preparing Data For Analysis", "body": "On 08-Jun-07 08:27:21, Christophe Pallier wrote:\n> Hi,\n> \n> Can you provide examples of data formats that are problematic\n> to read and clean with R ?\n> \n> The only problematic cases I have encountered were cases with\n> multiline and/or  varying length records (optional information).\n> Then, it is sometimes a good idea to preprocess the data to\n> present in a tabular format (one record per line).\n> \n> For this purpose, I use awk (e.g.\n> http://www.vectorsite.net/tsawk.html),\n> which is very adept at processing ascii data files  (awk is\n> much simpler to learn than perl, spss, sas, ...).\n\nI want to join in with an enthusiastic \"Me too!!\". For anything\nwhich has to do with basic checking for the kind of messes that\npeople can get data into when they \"put it on the computer\",\nI think awk is ideal. It is very flexible (far more so than\nmany, even long-time, awk users suspect), very transparent\nin its programming language (as opposed to say perl), fast,\nand with light impact on system resources (rare delight in\nthese days, when upgrading your software may require upgrading\nyour hardware).\n\nAlthough it may seem on the surface that awk is \"two-dimensional\"\nin its view of data (line by line, and per field in a line),\nit has some flexible internal data structures and recursive\nfunction capability, which allows a lot more to be done with\nthe data that have been read in.\n\nFor example, I've used awk to trace ancestry through a genealogy,\ngiven a data file where each line includes the identifier of an\nindividual and the identifiers of its male and female parents\n(where known). And that was for pedigree dogs, where what happens\nin real life makes Oedipus look trivial.\n\n> I have never encountered a data file in ascii format that I\n> could not reformat with Awk.  With binary formats, it is\n> another story...\n\nBut then it is a good idea to process the binary file using an\ninstance of the creating software, to produce a ASCII file (say\nin CSV format).\n\n> But, again, this is my limited experience; I would like to\n> know if there are situations where using SAS/SPSS is really\n> a better approach.\n\nThe main thing often useful for data cleaning that awk does\nnot have is any associated graphics. It is -- by design -- a\nline-by-line text-file processor. While, for instance, you\ncould use awk to accumulate numerical histogram counts, you\nwould have to use something else to display the histogram.\nAnd for scatter-plots there's probably not much point in\nbringing awk into the picture at all (unless a preliminary\nfiltration of mess is needed anyway).\n\nThat being said, though, there can still be a use to extract\ndata fields from a file for submission to other software.\n\nAnother kind of area where awk would not have much to offer\nis where, as a part of your preliminary data inspection,\nyou want to inspect the results of some standard statistical\nanalyses.\n\nAs a final comment, utilities like awk can be used far more\nfruitfully on operating systems (the unixoid family) which\nincorporate at ground level the infrastructure for \"plumbing\"\ntogether streams of data output from different programs.\n\nTed.\n\n--------------------------------------------------------------------\nE-Mail: (Ted Harding) \nFax-to-email: +44 (0)870 094 0861\nDate: 08-Jun-07                                       Time: 10:43:05\n------------------------------ XFMail ------------------------------\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}