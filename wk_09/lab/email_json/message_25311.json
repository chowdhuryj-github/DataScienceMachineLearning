{"category": "ham", "to_address": "\"Paul Smith\" <phhs80@gmail.com>", "from_address": "Jasjeet Singh Sekhon <sekhon@berkeley.edu>", "subject": "Re: [R] Bad optimization solution", "body": "\nThe issue is that you are using a derivative based optimizer for a\nproblem for which it is well known that such optimizers will not\nperform well.  You should consider using a global optimizer.  For\nexample, \"rgenoud\" combines a genetic search algorithm with a BFGS\noptimizer and it works well for your problem:\n\nlibrary(rgenoud)\n\nmyfunc <- function(x) {\n  x1 <- x[1]\n   x2 <- x[2]\n   abs(x1-x2)\n }\n\noptim(c(0.5,0.5),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=list(fnscale=-1))\n\ngenoud(myfunc, nvars=2, Domains=rbind(c(0,1),c(0,1)),max=TRUE,boundary.enforcement=2)\n\nmyfunc <- function(x) {\n  x1 <- x[1]\n  x2 <- x[2]\n  (x1-x2)^2\n}\n\noptim(c(0.2,0.2),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=list(fnscale=-1))\ngenoud(myfunc, nvars=2, Domains=rbind(c(0,1),c(0,1)),max=TRUE,boundary.enforcement=2)\n\nCheers,\nJas.\n\n=======================================\nJasjeet S. Sekhon                     \n                                      \nAssociate Professor             \nTravers Department of Political Science\nSurvey Research Center          \nUC Berkeley                     \n\nhttp://sekhon.berkeley.edu/\nV: 510-642-9974  F: 617-507-5524\n=======================================\n\n\n\n\n\n\nPaul Smith writes:\n > It seems that there is here a problem of reliability, as one never\n > knows whether the solution provided by R is correct or not. In the\n > case that I reported, it is fairly simple to see that the solution\n > provided by R (without any warning!) is incorrect, but, in general,\n > that is not so simple and one may take a wrong solution as a correct\n > one.\n > \n > Paul\n > \n > \n > On 5/8/07, Ravi Varadhan  wrote:\n > > Your function, (x1-x2)^2, has zero gradient at all the starting values such\n > > that x1 = x2, which means that the gradient-based search methods will\n > > terminate there because they have found a critical point, i.e. a point at\n > > which the gradient is zero (which can be a maximum or a minimum or a saddle\n > > point).\n > >\n > > However, I do not why optim converges to the boundary maximum, when analytic\n > > gradient is supplied (as shown by Sundar).\n > >\n > > Ravi.\n > >\n > > ----------------------------------------------------------------------------\n > > -------\n > >\n > > Ravi Varadhan, Ph.D.\n > >\n > > Assistant Professor, The Center on Aging and Health\n > >\n > > Division of Geriatric Medicine and Gerontology\n > >\n > > Johns Hopkins University\n > >\n > > Ph: (410) 502-2619\n > >\n > > Fax: (410) 614-9625\n > >\n > > Email: rvaradhan@jhmi.edu\n > >\n > > Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html\n > >\n > >\n > >\n > > ----------------------------------------------------------------------------\n > > --------\n > >\n > >\n > > -----Original Message-----\n > > From: r-help-bounces@stat.math.ethz.ch\n > > [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of Paul Smith\n > > Sent: Monday, May 07, 2007 6:26 PM\n > > To: R-help\n > > Subject: Re: [R] Bad optimization solution\n > >\n > > On 5/7/07, Paul Smith  wrote:\n > > > > I think the problem is the starting point.  I do not remember the\n > > details\n > > > > of the BFGS method, but I am almost sure the (.5, .5) starting point is\n > > > > suspect, since the abs function is not differentiable at 0.  If you\n > > perturb\n > > > > the starting point even slightly you will have no problem.\n > > > >\n > > > >              \"Paul Smith\"\n > > > >               > > >              >\n > > To\n > > > >              Sent by:                  R-help \n > > > >              r-help-bounces@st\n > > cc\n > > > >              at.math.ethz.ch\n > > > >\n > > Subject\n > > > >                                        [R] Bad optimization solution\n > > > >              05/07/2007 04:30\n > > > >              PM\n > > > >\n > > > >\n > > > >\n > > > >\n > > > >\n > > > >\n > > > >\n > > > >\n > > > > Dear All\n > > > >\n > > > > I am trying to perform the below optimization problem, but getting\n > > > > (0.5,0.5) as optimal solution, which is wrong; the correct solution\n > > > > should be (1,0) or (0,1).\n > > > >\n > > > > Am I doing something wrong? I am using R 2.5.0 on Fedora Core 6 (Linux).\n > > > >\n > > > > Thanks in advance,\n > > > >\n > > > > Paul\n > > > >\n > > > > ------------------------------------------------------\n > > > > myfunc <- function(x) {\n > > > >   x1 <- x[1]\n > > > >   x2 <- x[2]\n > > > >   abs(x1-x2)\n > > > > }\n > > > >\n > > > >\n > > optim(c(0.5,0.5),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=\n > > list(fnscale=-1))\n > > >\n > > > Yes, with (0.2,0.9), a correct solution comes out. However, how can\n > > > one be sure in general that the solution obtained by optim is correct?\n > > > In ?optim says:\n > > >\n > > >      Method '\"L-BFGS-B\"' is that of Byrd _et. al._ (1995) which allows\n > > >      _box constraints_, that is each variable can be given a lower\n > > >      and/or upper bound. The initial value must satisfy the\n > > >      constraints. This uses a limited-memory modification of the BFGS\n > > >      quasi-Newton method. If non-trivial bounds are supplied, this\n > > >      method will be selected, with a warning.\n > > >\n > > > which only demands that \"the initial value must satisfy the constraints\".\n > >\n > > Furthermore, X^2 is everywhere differentiable and notwithstanding the\n > > reported problem occurs with\n > >\n > > myfunc <- function(x) {\n > >   x1 <- x[1]\n > >   x2 <- x[2]\n > >   (x1-x2)^2\n > > }\n > >\n > > optim(c(0.2,0.2),myfunc,lower=c(0,0),upper=c(1,1),method=\"L-BFGS-B\",control=\n > > list(fnscale=-1))\n > >\n > > Paul\n > >\n > > ______________________________________________\n > > R-help@stat.math.ethz.ch mailing list\n > > https://stat.ethz.ch/mailman/listinfo/r-help\n > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html\n > > and provide commented, minimal, self-contained, reproducible code.\n > >\n > \n >\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}