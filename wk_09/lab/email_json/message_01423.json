{"category": "ham", "to_address": "\"Prof Brian Ripley\" <ripley@stats.ox.ac.uk>", "from_address": "ronggui <ronggui.huang@gmail.com>", "subject": "Re: [R] When to use quasipoisson instead of poisson family", "body": "On 4/10/07, Prof Brian Ripley  wrote:\n> On Tue, 10 Apr 2007, Achim Zeileis wrote:\n>\n> > On Tue, 10 Apr 2007, ronggui wrote:\n> >\n> >> It seems that MASS suggest to judge on the basis of\n> >> sum(residuals(mode,type=\"pearson\"))/df.residual(mode).\n>\n> Not really; that is the conventional moment estimator of overdispersion\n> and it does not suffer from the severe biases the unreferenced estimate\n> below has (and are illustrated in MASS).\n>\n> >> My question: Is\n> >> there any rule of thumb of the cutpoiont value?\n> >>\n> >> The paper \"On the Use of Corrections for Overdispersion\"\n>\n> Whose paper?  It is churlish not to give credit, and unhelpful to your\n> readers not to give a proper citation.\n\nThanks for pointing this out. There is the citation:\n@article{lindsey1999,\n  title={On the use of corrections for overdispersion},\n  author={Lindsey, JK},\n  journal={Applied Statistics},\n  volume={48},\n  number={4},\n  pages={553--561},\n  year={1999},\n  }\n\n> >> suggests overdispersion exists if the deviance is at least twice the\n> >> number of degrees of freedom.\n>\n> Overdispersion _exists_:  'all models are wrong but some are useful'\n> (G.E.P. Box).  The question is if it is important in your problem, not it\n> if is detectable.\n\n\n> > There are also formal tests for over-dispersion. I've implemented one for\n> > a package which is not yet on CRAN (code/docs attached), another one is\n> > implemented in odTest() in package \"pscl\". The latter also contains\n> > further count data regression models which can deal with both\n> > over-dispersion and excess zeros in count data. A vignette explaining the\n> > tools is about to be released.\n>\n> There are, but like formal tests for outliers I would not advise using\n> them, as you may get misleading inferences before they are significant,\n> and they can reject when the inferences from the small model are perfectly\n> adequate.\n>\n> In general, it is a much better idea to expand your models to take account\n> of the sorts of departures your anticipate rather than post-hoc test for\n> those departures and then if those tests do not fail hope that there is\n> little effect on your inferences.\n\nWhich is the better (or ) best way to expand the existing model?\nby adding some other relevant independent variables or by using other\nmore suitable model like \"Negative Binomial Generalized Linear Model\"?\n\nThanks!\n\n> The moment estimator \\phi of over-dispersion gives you an indication of\n> the likely effects on your inferences: e.g. estimated standard errors are\n> proportional to \\sqrt(\\phi).  Having standard errors which need inflating\n> by 40% seems to indicate that the rule you quote is too optimistic (even\n> when its estimator is reliable).\n>\n> --\n> Brian D. Ripley,                  ripley@stats.ox.ac.uk\n> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/\n> University of Oxford,             Tel:  +44 1865 272861 (self)\n> 1 South Parks Road,                     +44 1865 272866 (PA)\n> Oxford OX1 3TG, UK                Fax:  +44 1865 272595\n>\n\n\n-- \nRonggui Huang\nDepartment of Sociology\nFudan University, Shanghai, China\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}