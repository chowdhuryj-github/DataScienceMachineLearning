{"category": "ham", "to_address": "beginners@perl.org", "from_address": "\"John W. Krahn\" <krahnj@telus.net>", "subject": "Re: Efficient matching", "body": "Karyn Williams wrote:\n\n> I need to check for strings in a file. An example would be checking for a\n> username in /etc/passwd or /var/log/maillog. The string is in another\n> file, sometimes a single field on a line by itself, sometimes in a line\n> with other strings. Based on what I have seen online and what I have done\n> in other scripts I can :\n> \n> \n> 1. slurp the file into a single string and then check for a match.\n> \n> 2. read the relevent field from the file into hash keys and check for a\n> match.\n> \n> 3. Loop through the file line by line and look for a match.\n> \n> \n> Generally there are 1 - 2,000 strings, and 2,000 to 1,500,000 lines per\n> file.\n> \n> I am wondering how these different methods may impact performance.\n\nperldoc -q \"How do I efficiently match many regular expressions at once\"\n\n\n\nJohn\n\n-- \nTo unsubscribe, e-mail: beginners-unsubscribe@perl.org\nFor additional commands, e-mail: beginners-help@perl.org\nhttp://learn.perl.org/\n\n\n"}