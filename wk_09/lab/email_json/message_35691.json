{"category": "ham", "to_address": "=?windows-1252?Q?=3F=3F=3F?= <klijunjie@gmail.com>", "from_address": "Tony Plate <tplate@acm.org>", "subject": "Re: [R] R2 always increases as variables are added?", "body": "The answer to your question three is that the calculation of r-squared \nin summary.lm does depend on whether or not an intercept is included in \nthe model.  (Another part of the reason for you puzzlement is, I think, \nthat you are computing R-squared as SSR/SST, which is only valid when \nwhen the model has an intercept).\n\nThe code is in summary.lm, here are the relevant excerpts (assuming your \nmodel does not have weights):\n\n     r <- z$residuals\n     f <- z$fitted\n     w <- z$weights\n     if (is.null(w)) {\n         mss <- if (attr(z$terms, \"intercept\"))\n             sum((f - mean(f))^2)\n         else sum(f^2)\n         rss <- sum(r^2)\n     }\n...\n     ans$r.squared <- mss/(mss + rss)\n\nIf you want to compare models with and without an intercept based on \nR^2, then I suspect it's most appropriate to use the version of R^2 that \ndoes not use a mean.\n\nIt's also worthwhile thinking about what you are actually doing.  I find \nthe most intuitive definition of R^2 \n(http://en.wikipedia.org/wiki/R_squared) is\n\nR2 = 1 - SSE / SST\n\nwhere SSE = sum_i (yhat_i - y_i)^2, (sum of errors in predictions for \nyou model)\nand SST = sum_i (y_i - mean(y))^2 (sum of errors in predictions for an \nintercept-only model)\n\nThis means that the standard definition of R2 effectively compares the \nmodel with an intercept-only model.  As the error in predictions goes \ndown, R2 goes up, and the model that uses the mean(y) as a prediction \n(i.e., the intercept-only model) provides a scale for these errors.\n\nIf you think or know that the true mean of y is zero then it may be \nappropriate to compare against a zero model rather than an \nintercept-only model (in SST).  And if the sample mean of y is quite \ndifferent from zero, and you compare a no-intercept model against an \nintercept-only model, then you're going to get results that are not \neasily interpreted.\n\nNote that a common way of expressing and computing R^2 is as SSR/SST \n(which you used).  (Where SSR = sum_i (yhat_i - mean(y))^2 ). However, \nthis is only valid when the model has an intercept (i.e., SSR/SST = 1 - \nSSE/SST ONLY when the model has an intercept.)\n\nHere's some examples, based on your example:\n\n > set.seed(1)\n > data <- data.frame(x1=rnorm(10), x2=rnorm(10), y=rnorm(10), I=1)\n >\n > lm1 <- lm(y~1, data=data)\n > summary(lm1)$r.squared\n[1] 0\n > y.hat <- fitted(lm1)\n > sum((y.hat-mean(data$y))^2)/sum((data$y-mean(data$y))^2)\n[1] 5.717795e-33\n >\n > # model with no intercept\n > lm2 <- lm(y~x1+x2-1, data=data)\n > summary(lm2)$r.squared\n[1] 0.6332317\n > y.hat <- fitted(lm2)\n > # no-intercept version of R^2 (2 ways to compute)\n > 1-sum((y.hat-data$y)^2)/sum((data$y)^2)\n[1] 0.6332317\n > sum((y.hat)^2)/sum((data$y)^2)\n[1] 0.6332317\n > # standard (assuming model has intercept) computations for R^2:\n > SSE <- sum((y.hat - data$y)^2)\n > SST <- sum((data$y - mean(data$y))^2)\n > SSR <- sum((y.hat - mean(data$y))^2)\n > 1 - SSE/SST\n[1] 0.6252577\n > # Note that SSR/SST != 1 - SSE/SST (because the model doesn't have an \nintercept)\n > SSR/SST\n[1] 0.6616612\n >\n > # model with intercept included in data\n > lm3 <- lm(y~x1+x2+I-1, data=data)\n > summary(lm3)$r.squared\n[1] 0.6503186\n > y.hat <- fitted(lm3)\n > # no-intercept version of R^2 (2 ways to compute)\n > 1-sum((y.hat-data$y)^2)/sum((data$y)^2)\n[1] 0.6503186\n > sum((y.hat)^2)/sum((data$y)^2)\n[1] 0.6503186\n > # standard (assuming model has intercept) computations for R^2:\n > SSE <- sum((y.hat - data$y)^2)\n > SST <- sum((data$y - mean(data$y))^2)\n > SSR <- sum((y.hat - mean(data$y))^2)\n > 1 - SSE/SST\n[1] 0.6427161\n > SSR/SST\n[1] 0.6427161\n >\n >\n\nhope this helps,\n\nTony Plate\n\nDisclaimer: I too do not have any degrees in statistics, but I'm 95% \nsure the above is mostly correct :-)  If there are any major mistakes, \nI'm sure someone will point them out.\n\n??? wrote:\n> Hi, everybody,\n> \n> 3 questions about R-square:\n> ---------(1)----------- Does R2 always increase as variables are added?\n> ---------(2)----------- Does R2 always greater than 1?\n> ---------(3)----------- How is R2 in summary(lm(y~x-1))$r.squared\n> calculated? It is different from (r.square=sum((y.hat-mean\n> (y))^2)/sum((y-mean(y))^2))\n> \n> I will illustrate these problems by the following codes:\n> ---------(1)-----------  R2  doesn't always increase as variables are added\n> \n>> x=matrix(rnorm(20),ncol=2)\n>> y=rnorm(10)\n>>\n>> lm=lm(y~1)\n>> y.hat=rep(1*lm$coefficients,length(y))\n>> (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> [1] 2.646815e-33\n>> lm=lm(y~x-1)\n>> y.hat=x%*%lm$coefficients\n>> (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> [1] 0.4443356\n>> ################ This is the biggest model, but its R2 is not the biggest,\n> why?\n>> lm=lm(y~x)\n>> y.hat=cbind(rep(1,length(y)),x)%*%lm$coefficients\n>> (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> [1] 0.2704789\n> \n> \n> ---------(2)-----------  R2  can greater than 1\n> \n>> x=rnorm(10)\n>> y=runif(10)\n>> lm=lm(y~x-1)\n>> y.hat=x*lm$coefficients\n>> (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> [1] 3.513865\n> \n> \n>  ---------(3)----------- How is R2 in summary(lm(y~x-1))$r.squared\n> calculated? It is different from (r.square=sum((y.hat-mean\n> (y))^2)/sum((y-mean(y))^2))\n>> x=matrix(rnorm(20),ncol=2)\n>> xx=cbind(rep(1,10),x)\n>> y=x%*%c(1,2)+rnorm(10)\n>> ### r2 calculated by lm(y~x)\n>> lm=lm(y~x)\n>> summary(lm)$r.squared\n> [1] 0.9231062\n>> ### r2 calculated by lm(y~xx-1)\n>> lm=lm(y~xx-1)\n>> summary(lm)$r.squared\n> [1] 0.9365253\n>> ### r2 calculated by me\n>> y.hat=xx%*%lm$coefficients\n>> (r.square=sum((y.hat-mean(y))^2)/sum((y-mean(y))^2))\n> [1] 0.9231062\n> \n> \n> Thanks a lot for any cue:)\n> \n> \n> \n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}