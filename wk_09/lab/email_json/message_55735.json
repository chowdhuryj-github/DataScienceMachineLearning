{"category": "ham", "to_address": "ivo welch <ivowel@gmail.com>", "from_address": "Prof Brian Ripley <ripley@stats.ox.ac.uk>", "subject": "Re: [R] Memory Experimentation: Rule of Thumb = 10-15 Times the\n Memory", "body": "The R Data Import/Export Manual points out several ways in which you can\nuse read.csv more efficiently.\n\nOn Tue, 26 Jun 2007, ivo welch wrote:\n\n> dear R experts:\n>\n> I am of course no R experts, but use it regularly.  I thought I would\n> share some experimentation  with memory use.  I run a linux machine\n> with about 4GB of memory, and R 2.5.0.\n>\n> upon startup, gc() reports\n>\n>         used (Mb) gc trigger (Mb) max used (Mb)\n> Ncells 268755 14.4     407500 21.8   350000 18.7\n> Vcells 139137  1.1     786432  6.0   444750  3.4\n>\n> This is my baseline.  linux 'top' reports 48MB as baseline.  This\n> includes some of my own routines that are always loaded.  Good..\n>\n>\n> Next, I created a s.csv file with 22 variables and 500,000\n> observations, taking up an uncompressed disk space of 115MB.  The\n> resulting object.size() after a read.csv() is 84,002,712 bytes (80MB).\n>\n>> s= read.csv(\"s.csv\");\n>> object.size(s);\n>\n> [1] 84002712\n>\n>\n> here is where things get more interesting.  after the read.csv() is\n> finished, gc() reports\n>\n>           used (Mb) gc trigger  (Mb) max used  (Mb)\n> Ncells   270505 14.5    8349948 446.0 11268682 601.9\n> Vcells 10639515 81.2   34345544 262.1 42834692 326.9\n>\n> I was a big surprised by this---R had 928MB intermittent memory in\n> use.  More interestingly, this is also similar to what linux 'top'\n> reports as memory use of the R process (919MB, probably 1024 vs. 1000\n> B/MB), even after the read.csv() is finished and gc() has been run.\n> Nothing seems to have been released back to the OS.\n>\n> Now,\n>\n>> rm(s)\n>> gc()\n>         used (Mb) gc trigger  (Mb) max used  (Mb)\n> Ncells 270541 14.5    6679958 356.8 11268755 601.9\n> Vcells 139481  1.1   27476536 209.7 42807620 326.6\n>\n> linux 'top' now reports 650MB of memory use (though R itself uses only\n> 15.6Mb).  My guess is that It leaves the trigger memory of 567MB plus\n> the base 48MB.\n>\n>\n> There are two interesting observations for me here:  first, to read a\n> .csv file, I need to have at least 10-15 times as much memory as the\n> file that I want to read---a lot more than the factor of 3-4 that I\n> had expected.  The moral is that IF R can read a .csv file, one need\n> not worry too much about running into memory constraints lateron.  {R\n> Developers---reducing read.csv's memory requirement a little would be\n> nice.  of course, you have more than enough on your plate, already.}\n>\n> Second, memory is not returned fully to the OS.  This is not\n> necessarily a bad thing, but good to know.\n>\n> Hope this helps...\n>\n> Sincerely,\n>\n> /iaw\n>\n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html\n> and provide commented, minimal, self-contained, reproducible code.\n>\n\n-- \nBrian D. Ripley,                  ripley@stats.ox.ac.uk\nProfessor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/\nUniversity of Oxford,             Tel:  +44 1865 272861 (self)\n1 South Parks Road,                     +44 1865 272866 (PA)\nOxford OX1 3TG, UK                Fax:  +44 1865 272595\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}