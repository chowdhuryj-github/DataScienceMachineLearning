{"category": "ham", "to_address": "\"'hadley wickham'\" <h.wickham@gmail.com>,\n   \"'Wensui Liu'\" <liuwensui@gmail.com>", "from_address": "Bert Gunter <gunter.berton@gene.com>", "subject": "Re: [R] Neural Nets (nnet) - evaluating success rate of predictions", "body": "Folks:\n\nIf I understand correctly, the following may be pertinent.\n\nNote that the procedure:\n\nmin.nnet = nnet[k] such that error rate of nnet[k] = min[i] {error\nrate(nnet(training data) from ith random start) }\n\ndoes not guarantee a classifier with a lower error rate on **new** data than\nany single one of the random starts. That is because you are using the same\ntraining set to choose the model (= nnet parameters) as you are using to\ndetermine the error rate. I know it's tempting to think that choosing the\nbest among many random starts always gets you a better classifier, but it\nneed not. The error rate on the training set for any classifier -- be it a\nsingle one or one derived in some way from many -- is a biased estimate of\nthe true error rate, so that choosing a classifer on this basis does not\nassure better performance for future data. In particular, I would guess that\nchoosing the best among many (hundreds/thousands) random starts is probably\nalmost guaranteed to produce a poor predictor (ergo the importance of\nparsimony/penalization).  I would appreciate comments from anyone, pro or\ncon, with knowledge and experience of these things, however, as I'm rather\nlimited on both.\n\nThe simple answer to the question of obtaining the error rate using\nvalidation data is: Do whatever you like to choose/fit a classifier on the\ntraining set. **Once you are done,** the estimate of your error rate is the\nerror rate you get on applying that classifier to the validation set. But\nyou can do this only once! If you don't like that error rate and go back to\nfinding a a better predictor in some way, then your validation data have now\nbeen used to derive the classifier and thus has become part of the training\ndata, so any further assessment of the error rate of a new classifier on it\nis now also a biased estimate. You need yet new validation data for that.\n\nOf course, there are all sort of cross validation schemes one can use to\navoid -- or maybe mitigate -- these issues: most books on statistical\nclassification/machine learning discuss this in detail.\n\n\nBert Gunter\nGenentech Nonclinical Statistics\n\n\n-----Original Message-----\nFrom: r-help-bounces@stat.math.ethz.ch\n[mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of hadley wickham\nSent: Monday, May 07, 2007 5:26 AM\nTo: Wensui Liu\nCc: r-help@stat.math.ethz.ch\nSubject: Re: [R] Neural Nets (nnet) - evaluating success rate of predictions\n\nPick the one with the lowest error rate on your training data?\nHadley\n\nOn 5/7/07, Wensui Liu  wrote:\n> well, how to do you know which ones are the best out of several hundreds?\n> I will average all results out of several hundreds.\n>\n> On 5/7/07, hadley wickham  wrote:\n> > On 5/6/07, nathaniel Grey  wrote:\n> > > Hello R-Users,\n> > >\n> > > I have been using (nnet) by Ripley  to train a neural net on a test\ndataset, I have obtained predictions for a validtion dataset using:\n> > >\n> > > PP<-predict(nnetobject,validationdata)\n> > >\n> > > Using PP I can find the -2 log likelihood for the validation datset.\n> > >\n> > > However what I really want to know is how well my nueral net is doing\nat classifying my binary output variable. I am new to R and I can't figure\nout how you can assess the success rates of predictions.\n> > >\n> >\n> > table(PP, binaryvariable)\n> > should get you started.\n> >\n> > Also if you're using nnet with random starts, I strongly suggest\n> > taking the best out of several hundred (or maybe thousand) trials - it\n> > makes a big difference!\n> >\n> > Hadley\n> >\n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide\nhttp://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> >\n>\n>\n> --\n> WenSui Liu\n> A lousy statistician who happens to know a little programming\n> (http://spaces.msn.com/statcompute/blog)\n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}