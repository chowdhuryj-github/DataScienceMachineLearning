{"category": "ham", "to_address": "\"Wilfred Zegwaard\" <wilfred.zegwaard@gmail.com>", "from_address": "\"Johann Hibschman\" <johannh@gmail.com>", "subject": "Re: [R] Reasons to Use R", "body": "On 4/6/07, Wilfred Zegwaard  wrote:\n\n> I'm not a programmer, but I have the experience that R is good for\n> processing large datasets, especially in combination with specialised\n> statistics.\n\nThis I find a little surprising, but maybe it's just a sign that I'm\nnot experienced enough with R yet.\n\nI can't use R for big datasets.  At all.  Big datasets take forever to\nload with read.table, R frequently runs out of memory,  and nlm or\ngnlm never seem to actually converge to answers.  By comparison, I can\npoint SAS and NLIN at this data without problem.  (Of course, SAS is\nrunning on a pretty powerful dedicated machine with a big ram disk, so\nthat may be part of the problem.)\n\nR's pass-by-value semantics also make it harder than it should be to\ndeal with where it's crucial that you not make a copy of the data\nframe, for fear of running out of memory.  Pass-by-reference would\nmake implementing data transformations so much easier that I don't\nreally understand how pass-by-value became the standard.  (If there's\na trick to doing in-place transformations, I've not found it.)\n\nRight now, I'm considering starting on a project involving some big\nMonte Carlo integrations over the complicated posterior parameter\ndistributions of a nonlinear regression model, and I have the strong\nfeeling that R will just choke.\n\nR's great for small projects, but as soon as you even a few hundred\nmegs of data, it seems to break down.\n\nIf I'm doing things wrong, please tell me.  :-)  SAS is a beast to work with.\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}