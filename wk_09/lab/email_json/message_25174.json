{"category": "ham", "to_address": "\"Bert Gunter\" <gunter.berton@gene.com>", "from_address": "\"hadley wickham\" <h.wickham@gmail.com>", "subject": "Re: [R] Neural Nets (nnet) - evaluating success rate of predictions", "body": "On 5/7/07, Bert Gunter  wrote:\n> Folks:\n>\n> If I understand correctly, the following may be pertinent.\n>\n> Note that the procedure:\n>\n> min.nnet = nnet[k] such that error rate of nnet[k] = min[i] {error\n> rate(nnet(training data) from ith random start) }\n>\n> does not guarantee a classifier with a lower error rate on **new** data than\n> any single one of the random starts. That is because you are using the same\n> training set to choose the model (= nnet parameters) as you are using to\n> determine the error rate. I know it's tempting to think that choosing the\n> best among many random starts always gets you a better classifier, but it\n> need not. The error rate on the training set for any classifier -- be it a\n> single one or one derived in some way from many -- is a biased estimate of\n> the true error rate, so that choosing a classifer on this basis does not\n> assure better performance for future data. In particular, I would guess that\n> choosing the best among many (hundreds/thousands) random starts is probably\n> almost guaranteed to produce a poor predictor (ergo the importance of\n> parsimony/penalization).  I would appreciate comments from anyone, pro or\n> con, with knowledge and experience of these things, however, as I'm rather\n> limited on both.\n\nI agree - it's never a good idea to use the same data for creating\nyour classifier and determining it's effectiveness (I meant to say\n\"pick the one with the lowest error rate on your TEST data\").\n\nThe reason to choose from many random starts is that fitting a given\nneural network _model_ (ie. input x, n nodes, ...) is very hard due to\nthe large overparameterisation of the problem space.  For example, the\nparameters for one node in a given layer can be exchanged with the\nparameters of another node (as well as the parameters that use those\nnodes in next layer), without changing the overall model.  This makes\nit very hard to optimise, and nnet in R often gets stuck in local\nminima.\nLooking at what individual nodes are doing, you often see examples\nwhere some nodes contribute nothing to the overall classification.\nThe random starts aren't to find different models but to find the\nparameters for the given model that fits best.  And following this\nline of argument, you would probably want to use the internal\ncriterion value, rather than some external measure of accuracy.\n\n> The simple answer to the question of obtaining the error rate using\n> validation data is: Do whatever you like to choose/fit a classifier on the\n> training set. **Once you are done,** the estimate of your error rate is the\n> error rate you get on applying that classifier to the validation set. But\n> you can do this only once! If you don't like that error rate and go back to\n> finding a a better predictor in some way, then your validation data have now\n> been used to derive the classifier and thus has become part of the training\n> data, so any further assessment of the error rate of a new classifier on it\n> is now also a biased estimate. You need yet new validation data for that.\n\nUnderstanding that that estimate is biased is important, but in\npractice, do people really care that much?  If you have looked at a\nsingle plot of your data and used that to inform your choice of the\nclassifier your estimates will already be biased (but if you have used\nother knowledge of the data or subject area, you might expect them to\nbe biased in a positive direction). Are the estimates of model really\nthe most important thing?  Surely an understanding of the problem/data\nis what you are really trying to gain.\n\nHadley\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}