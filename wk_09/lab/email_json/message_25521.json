{"category": "ham", "to_address": "<h.wickham@gmail.com>, <r-help@stat.math.ethz.ch>", "from_address": "\"S Ellison\" <S.Ellison@lgc.co.uk>", "subject": "Re: [R] Weighted least squares", "body": "Doubling the length of the data doubles the apparent number of observations. You would expect the standard error to reduce by sqrt(2) (which it just about does, though I'm not clear on why its not exact here)\n\nWeights are not as simple as they look. You have given all your data the same weight, so the answer is independent of the weights (!). Try again with weights=rep(4,100) etc. Equal weights simply cancel out in the lm process. In fact, some linear regression algorithms rescale all weights to sum to 1; in others, weights are scaled to average 1; done 'naturally' the weights simply appear in two places which cancel out in the final covariance matrix calculation (eg in the weighted 'residual sd' and in the hessian for the chi-squared function, if I remember correctly). \n\nBottom line - equal weights make no difference in lm, so choose what you like. 1 is a good number, though.\n\nSteve e\n\n>>> \"hadley wickham\"  08/05/2007 10:08:34 >>>\nDear all,\n\nI'm struggling with weighted least squares, where something that I had\nassumed to be true appears not to be the case.  Take the following\ndata set as an example:\n\n\n*******************************************************************\nThis email and any attachments are confidential. Any use, co...{{dropped}}\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}