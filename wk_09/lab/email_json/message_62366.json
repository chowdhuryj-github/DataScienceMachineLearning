{"category": "ham", "to_address": "Peter Dalgaard <p.dalgaard@biostat.ku.dk>", "from_address": "Martin Morgan <mtmorgan@fhcrc.org>", "subject": "Re: [R] Lookups in R", "body": "Michael,\n\nA hash provides constant-time access, though the resulting perl-esque\ndata structures (a hash of lists, e.g.) are not convenient for other\nmanipulations\n\n> n_accts <- 10^3\n> n_trans <- 10^4\n> t <- list()\n> t$amt <- runif(n_trans)\n> t$acct <- as.character(round(runif(n_trans, 1, n_accts)))\n> \n> uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)\n> ## keys, presumably account ids\n> for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0, n=0)\n> \n> system.time(for (i in seq_along(t$amt)) {\n+     acct <- t$acct[i]\n+     x <- uhash[[acct]]\n+     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)\n+ })\n   user  system elapsed \n  0.264   0.000   0.262 \n> udf <- data.frame(amt=0, n=rep(0L, n_accts),\n+                   row.names=as.character(1:n_accts))\n> system.time(for (i in seq_along(t$amt)) {\n+     idx <- row.names(udf)==t$acct[i]\n+     udf[idx, ] <- c(udf[idx,\"amt\"], udf[idx, \"n\"]) + c(t$amt[i], 1)\n+ })\n   user  system elapsed \n 18.398   0.000  18.394 \n\nPeter Dalgaard  writes:\n\n> mfrumin wrote:\n>> Hey all; I'm a beginner++ user of R, trying to use it to do some processing\n>> of data sets of over 1M rows, and running into a snafu.  imagine that my\n>> input is a huge table of transactions, each linked to a specif user id.  as\n>> I run through the transactions, I need to update a separate table for the\n>> users, but I am finding that the traditional ways of doing a table lookup\n>> are way too slow to support this kind of operation.\n>>\n>> i.e:\n>>\n>> for(i in 1:1000000) {\n>>    userid = transactions$userid[i];\n>>    amt = transactions$amounts[i];\n>>    users[users$id == userid,'amt'] += amt;\n>> }\n>>\n>> I assume this is a linear lookup through the users table (in which there are\n>> 10's of thousands of rows), when really what I need is O(constant time), or\n>> at worst O(log(# users)).\n>>\n>> is there any way to manage a list of ID's (be they numeric, string, etc) and\n>> have them efficiently mapped to some other table index?\n>>\n>> I see the CRAN package for SQLite hashes, but that seems to be going a bit\n>> too far.\n>>   \n> Sometimes you need a bit of lateral thinking. I suspect that you could \n> do it like this:\n>\n> tbl <- with(transactions, tapply(amount, userid, sum))\n> users$amt <- users$amt + tbl[users$id]\n>\n> one catch is that there could be users with no transactions, in which \n> case you may need to replace userid by factor(userid, levels=users$id). \n> None of this is tested, of course.\n>\n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html\n> and provide commented, minimal, self-contained, reproducible code.\n\n-- \nMartin Morgan\nBioconductor / Computational Biology\nhttp://bioconductor.org\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}