{"category": "ham", "to_address": "Lars Marowsky-Bree <lmb@suse.de>", "from_address": "tridge@samba.org", "subject": "Re: [Linux-ha-dev] CTDB and clustered Samba", "body": "Lars,\n\n > OK. So you're using the CFS as a DLM. Expensive, but possible. ;-)\n\nno, the DLM in CTDB is on our own transport. The single lock is\ngained once when a node becomes the recovery master. That means one\nlock every few days in a typical setup, which is hardly expensive!\n\n > (That also means that with OCFS2, you can't do that; it doesn't have\n > cluster-wide flock yet, _but_ it allows you to access it's DLM via\n > dlmfs, so that could be used.)\n\nyes, we could support OCFS2 by having a config option to dispense with\nthis lock, as its really a paranoia check. CTDB already has its own\nelection code, so the lock in the filesystem is a double check for\nsplit-brain scenarious and to prevent race conditions at startup when\nour transport is still establishing itself.\n\nI'm hoping some OCFS2 developers will get involved a bit with CTDB and\nrecommend a better approach. In the meantime you would need to use\nsome other distributed filesystem (even NFS) for this lock, or disable\nthat lock in the code.\n\n > No, not at all. DLM usually also cache locks mastered locally.\n > \n > (I know your lock semantics are very complex, but I'm just sayin' ;-)\n\nyes, many of them do, although often quite slowly, and when there is\nlock contention the performance drops a lot.\n\nSee for example the ping_pong.c test http://junkcode.samba.org/#ping_pong. Run that\non several nodes at once and the performance is awful.\n\n > After an fsync, or with O_DIRECT etc, sure, but otherwise, no.\n\nyou would think so, but testing showed its horribly slow. Run\nsomething like our tdb test tool on a cluster filesystem and its\nterrible. Run on more than one node at a time and it gets even worse.\n\nIf you look at page 17 of this presentation:\n\n  http://samba.org/~tridge/sambaxp-07/ctdb.pdf\n\nthen you can see what sort of results we were getting when we let the\ncluster filesystem do all the work, and what we're getting now (the\nsuper-linear speedup is due to cache effects on GPFS). The results\ndiffered between cluster filesystems, but none were good.\n\nThe non-CTDB results are a bit misleading because they leave the whole\ntdb in one file on the cluster (which is normally how a tdb is\ndone). Volker experimented with other schemes where he split out each\nrecord into a separate file, and that improved things, but to nothing\nlike the degree that CTDB does.\n\n > Well, attaching the page to the DLM reply makes that replication\n > basically free, at least in terms of latency.\n\nIs this really done a page at a time?\n\n > Sure, that makes sense. Can you elaborate on this a bit more?\n\nok, we can look at one of the most heavily used databases in Samba,\nwhich is brlock.tdb. That database maps windows byte range locking\nsemantics onto posix byte range locking semantics. Every time a\nwindows client does a read or write operation this database needs to\nbe checked (as windows locks are mandatory, whereas posix are\nadvisory).\n\nSo, we have a record in that database per open file on the\ncluster. The record is keyed by device:inode (or for some cluster\nfilesystems fsid:inode or fsname:inode). Inside the record is a set of\nsub-records which describe the windows byte range locks held by all\nclients. The sub-records are tagged with an id saying which instance\nof Samba put it there (nodenumber:pid). On each IO operation, Samba\nneeds to look inside that record and see if the read/write conflicts\nwith an existing lock.\n\nNow what happens if a node goes down? The open file handles of that\nnode are lost (they will need to be re-established by clients when\nthey reconnect to a new node) which means the sub-records associated\nwith that node are no longer needed. \n\nWe cope with this in a recovery run, where the elected recovery master\nscans the whole db and looks for the instance of each record with the\nhighest 'RSN' (record serial number). That means it gathers the most\nrecent record it can find. That record is then scrubbed of any\nsub-records associated with the dead node.\n\nThis way we guarantee to get correct data for all of the sub-records\nassociated with any of the nodes that are still alive, and remove all\nof the sub-records associated with the dead nodes. So we've recovered\nall the data we need, but without at any time having to do any sort of\nreplication, and without having to write any data to shared storage.\n\n > OK, I'm not questioning your judgement and experience, I'm just trying\n > to understand why, and what we might be able to accomodate (as we're\n > looking at switching commlayers anyway). Or, if you're right, what we\n > could reuse - as the Linux HA v2 code has a \"CIB\" (cluster information\n > base) which is a replicated/distributed db thingy too, maybe we could\n > reuse parts of the CTDB or something ;-)\n\npossibly, though you would probably want a different data persistence\nmodel to us. We are planning on adding support for persistent data (so\nwe can't assume the above logic for sub-records) but that isn't done\nyet. It's much less important for Samba than our temporary data, as\nits the temporary data that is accessed a lot.\n\n > TCP in a controlled LAN environment likely performs pretty well, yes.\n > But you will open N:N connections in your cluster as well, no? Don't you\n > need some broadcasts?\n\ncurrently each node has 2*N sockets open, giving a total of 2*N*N\nacross the whole cluster.\n\nWe broadcast by sending to each node separately. Broadcasts aren't a\nbig problem though, as they are really only used for management tasks\n(such as by the recovery master). They aren't used for anything that\nis speed sensitive.\n\nInternally, ctdbd is event driven. It never makes any blocking system\ncalls, so when you broadcast you don't sit waiting for all the sockets\nto complete sending their data. \n\n > Or easier - is there a document I can read which outlines the CTDB\n > requirements?\n\nThere isn't a document on the internals yet apart from the code\nitself. \n\nWriting a backend is fairly simple though, you fill in a structure\nlike this:\n\nstatic const struct ctdb_methods ctdb_tcp_methods = {\n\t.initialise   = ctdb_tcp_initialise,\n\t.start        = ctdb_tcp_start,\n\t.queue_pkt    = ctdb_tcp_queue_pkt,\n\t.add_node     = ctdb_tcp_add_node,\n\t.allocate_pkt = ctdb_tcp_allocate_pkt,\n\t.shutdown     = ctdb_tcp_shutdown,\n};\n\nand each of those functions is fairly simple.\n\nThe backend needs to queue packets when the connection to the node is\ndown, and needs to handle errors pretty carefully, but nothing too\nhard. It does need to be careful to never do a blocking operation\nthough, and never to use a polling loop (the latter can be fatal with\nSCHED_FIFO). So for example it must use non-blocking connect()\ncalls. It also needs to use our event framework, so it can be called\nwhen something happens on one of the sockets. CTDB provides IO library\nroutines that make this a bit easier.\n\n > Right, just like TCP on TCP is a bad idea I expect this would be as\n > well. (Just like the misdesigned approach to try running heartbeat's\n > native comm layer on top of openAIS; it's the same stacking issue.)\n\nyep\n\n > If not, that's a conference paper I'd fly a few miles for to attend the\n > presentation ;-)\n\nI've given some papers (see my home page) but not in the level of\ndetail you would want. We do plan on writing up the internal design of\nCTDB, but for now I'm afraid the code is the only guide to the\ndetails.\n\nThere is an old design doc on our wiki\n(http://wiki.samba.org/index.php/CTDB_Project) but its pretty dated\nnow. It might confuse more than it enlightens :)\n\nCheers, Tridge\n\n"}