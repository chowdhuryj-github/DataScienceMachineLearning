{"category": "ham", "to_address": "\"Undisclosed.Recipients\": ;", "from_address": "chromatic <chromatic@wgz.org>", "subject": "Re: new FAQs", "body": "On Wednesday 23 May 2007 18:06:38 Will Coleda wrote:\n\n> I confess to not grasping the point you claim is simple.  As you\n> understand it, what is there about a register based machine, as\n> opposed to a stack based machine, that specifically improves the\n> performance of operating on dynamically typed data, without regard to\n> performance differences between the two architectures that are\n> independent of typing models?\n\nParrot has four types of registers, for integers, floating point numbers, \nstrings, and PMCs.\n\nWith hot-spot operations as you might find in number crunching code, Parrot \nand the underlying CPU may be able to operate on the values in registers \nalmost directly (that is, load from Parrot register into CPU register, store \nback) without having to marshall and demarshall all of the Parrot arguments \nfor a section of code into and out of a single stack.\n\nWithout particular benchmarks, however, it's difficult to give any particular \nperformance guidelines.\n\nFor a point of comparison, however, Perl 5 spends a lot of time in managing \nthe argument stack--a significant amount of time for otherwise small and fast \noperations.\n\n> It sounds like you are saying that languages are free to implement\n> their own semantics using their own code, and that they can choose not\n> to interoperate with predefined Parrot types or types from other\n> languages when that would negatively impact their goals, such as\n> performance. While that rings true, it seems that Parrot is not\n> providing that ability -- languages can already implement whatever\n> they want without Parrot.  And if languages are free to ignore\n> predefined and foreign types, when what benefit will they actually get\n> from Parrot?\n\n - better compiler tools than lex and yacc.\n - native support for plenty of dynamic language features, if they want them\n - whatever portability and maturity Parrot has at the time\n - interoperability to whatever degree they allow\n\n> Moreover, this does not address my initial question.  I am asking, to\n> rephrase it bluntly, \"If Parrot makes dynamic typing faster, doesn't\n> that have to make static typing slower?\"  That is, is Parrot making a\n> tradeoff here?\n\nOf course.  We can't do the C++ trick of compiling class attribute access into \nslot offset access.  The world doesn't cool to near absolute zero when \ncompile time ends.  Someday, someone may want to add an attribute to a class \nat runtime.\n\n> If it is, how large is the tradeoff and what is its \n> nature.\n\nNo benchmark; won't speculate.\n\n> If it is not, then why doesn't everyone else simply do what \n> you are doing and gain the same benefit?\n\nSome static compilers and environments do use runtime profiling to identify \noptimizations that static analysis was unable to discover.\n\n> It would seem that Parrot either has to be different from the JVM and\n> CLR due to design or implementation optimizations that favor a\n> specific typing model over others -- which is what it seems to claim --\n> or else it does not -- either it is not thus differently designed, or\n> it is not thus differently implemented.  If it does not, then it seems\n> inappropriate for it to make the claim -- and thus would raise the\n> question of why Parrot should be considered a superior target for\n> dynamically or statically typed language compilers.\n\nSnarkily, it's better than the JVM because it actually supports features of \ndynamic languages natively without forcing all dynamic languages built on it \nto invent everything besides \"look up named method at runtime\".\n\nIt's better than the CLR because the Parrot copyright holders don't have the \nplans or (as far as I know) the ability to bring patent suits against anyone \nwho uses Parrot.  Also our definition of portability is more than \"Both \nWindows XP AND Vista\".\n\n> What tradeoffs could Parrot be making that will have a significant\n> benefit for dynamically typed languages -- significant enough to\n> justify the creation of Parrot itself -- without significant detriment\n> to statically typed languages?  Again, if these tradeoffs are so\n> broadly beneficial, why would the JVM or CLR not simply implement them\n> themselves?\n\n> Most simply: What is being lost to gain whatever is being gained?\n\nStructural typing and related optimizations.\n\nAs for gains, it's not difficult for me to imagine that Parrot could support \nsomething like Smalltalk's browser.\n\n> If Parrot is designed to benefit of dynamically typed languages, how\n> will Parrot handle statically typed code in those languages.\n\nI don't understand this question.  Are they statically typed or dynamically \ntyped?\n\nNow if you're asking whether, as with some Lisp implementations, you can give \noptional type hints to the compiler so that it can perform optimizations, \nthen that's indeed a feature of Perl 6 and Parrot will support it somehow.\n\nWe haven't reached the point of worrying about optimizations, however.\n\n> Will Parrot discourage the use of static typing features in languages like\n> Perl by making that code execute more slowly or inefficiently than\n> equivalent dynamically typed code?\n\nNo benchmark; won't speculate.\n\n>  > > 2. General Features\n>  > >\n>  > > a. How will Parrot support reflection and attributes?\n>  > >\n>  > > b. How will Parrot support generics types?\n>  > >\n>  > > c. How will Parrot support interface types?\n>  > >\n>  > > d. What kind of security models will Parrot support?\n>  > >\n>  > > e. How will Parrot support small-footprint systems?\n>  >\n>  > Perhaps miniparrot can help take care of this.  If miniparrot's a\n>  > miniature parrot, and perhaps supporting only those features that\n\n> While many things are perhaps true, this answer sounds like \"There is\n> no definite plan for supporting this.\"\n\nMore charitably you might say \"There are no concrete designs for these systems \nyet.\"\n\n>  > > f. How will Parrot support direct access to \"unmanaged\" resources?\n\n>  > Is this like UnmanagedStruct?\n\n> I mean supporting direct access to the underlying address space and\n> support for determining the sizes of data within that memory.  For\n> example, direct access to a framebuffer.\n\nThis is UnManagedStruct.\n\n>  > > g. How will Parrot facilitate distributed processing?\n>  >\n>  > With native threading support.\n>\n> I think you misunderstood my question.  By \"distributed\", I meant the\n> execution of code in multiple address spaces, or the non-concurrent\n> execution of code.  What support will Parrot provide for migrating\n> data or code between environment with different byte orders.  How will\n> Parrot support capturing execution state into a preservable or\n> transportable form?\n\nUnspecified.\n\n> Again, this does not seem to be clear, so I will provide an\n> example. If a Perl compiler is compiling Perl code, and that code is\n> written to increment the result of a call into some Python code that\n> returns a PythonString, how can the compiler ask the PythonString PMC\n> if it implements the \"increment\", so that it can detect at compile\n> time what the behavior of the statement will be?\n\nI don't believe it would do so at compile time.  Neither Python nor Perl do \nthis to my knowledge.\n\n> More broadly, how can statically typed code determine if the values\n> produced by an operation will conform to the type requirements?\n\nI suppose it would perform type analysis on some sort of abstract tree \nstructure before generating Parrot bytecode.\n\n> What are \"basic things\"?\n\nPrimitive handling operations.\nAggreggate access.\nMethod lookup and invocation.\n\n> What if a language inherently differs in how \n> it handles those things?\n\nThen their semantics differ, and you can only rely on their documented \ninterfaces for appropriate behavior, the same way that you can only rely on \nthe documented interface of objects outside of your control.\n\n> For example, incrementing a scalar would \n> seem to be a basic operation in Perl, but Python will not implement\n> that basic thing in the same way.  It would seem that one or both\n> sides of this cross-language exchange of very basic types of data will\n> be problematic.\n\nI don't see why.  Certainly you have to be cognizant if you're crossing a \nlanguage boundary, but all of the cross-language code I've written so far has \nworked without problems.\n\n> You say \"the best way for parrot\" -- how can Parrot have a judgmental\n> reference point independent from the languages that target it and the\n> users of those languages?\n\nI don't understand the question.  The point of view of Parrot is \"What \nbehavior does Parrot need to support to be a good host for dynamic \nlanguages?\"  That's not at all independent of target languages.\n\n> You say \"No\" initially, but then go on to say \"yes\" in substance.  If\n> the PMCs are responsible for this, and if languages provide the PMCs,\n> then the languages are responsible for this.\n>\n> To explicitly state what is implied by this question.  If every\n> language must provide PMCs that understand how to interact with types\n> of other languages, then languages will only be able to interact with\n> each other to the degree that one or both of those languages provides\n> support.  For Perl to use data returned from Python code, either Perl\n> will have to recognize Python types or Python will have to know to\n> produce Perl types.  Then for Perl to call Tcl code, Perl and/or Tcl\n> will have to be taught about each other.  And then for Python to call\n> Tcl, yet additional code will need to be created.  Indeed, it could be\n> necessary for Python code to call Perl code that calls Tcl code,\n> because Perl might understand how to handle a Tcl type that Python\n> does not.  And the more languages that are added, the more types each\n> language will be asked to implement code to interact with.\n>\n> This seems like a scalability problem.\n\nThus, vtables.\n\nAll string-like PMCs implement a set of string vtable entries.  All access to \nstring data goes through those vtables.\n\nA GroovyString PMC that performs the \"clear string\" operation when code \ninvokes its \"concatenate\" vtable entry is buggy in the same way that it would \nbe if it launched Nethack instead.\n\nFor a language to interoperate with other languages through its PMCs, those \nPMCs must adhere to the semantics of the appropriate PMC vtable interfaces.\n\n> This would mean that any cross-language code could generate runtime\n> exceptions in operations that otherwise are generally considered not to\n> be able to fail.  Indeed, it would seem that every possible operation\n> would possibly fail at runtime when handling foreign data.\n\nI think you're overstating the case.\n\n> This would seem to strongly discourage multi-language programming --\n> to the point of it never happening.\n\nDitto.\n\n> What will Parrot do to make this acceptable?  Will end-users be forced\n> to write their own test cases that attempt all valid combinations of\n> all data between all languages they wish to use?\n\nI don't see it as such a problem.  I can imagine that Parrot could detect \ncross-language calls and morph variables appropriately, but I have some \namount of confidence that the vtable interfaces are well-chosen.\n\n\n> Now, this was not the best of examples in the first place, because I\n> would not argue that 'ToString' is not the kind of really-useful thing\n> you want in a core data type.  The essential meaning of the routine\n> being \"make something a human can read\" -- and humans are the people\n> using the machines.  But, as you can see, there was no need for the\n> core data type to provide me with an implemented 'addValue' -- it can\n> simply be layered on using a more primitive and extensible runtime\n> support for properties.\n\nOf course, and we could suggest in a very JVM-ish way that all dynamic \nlanguages should roll their own string concatenation operators (hey, just two \nstring fetches, a splice, and a store!).\n\nThat's not Parrot's goal, however.\n\n> I don't see the simplicity or the speed benefit.  I do see the memory\n> cost.  If anything, I suspect that these larger objects will fill a\n> CPU cache faster and be slower to load because of this increased size,\n> leading to slower runtime performance.\n\nNo benchmark; won't speculate beyond saying that the object structure only \nneeds a pointer to a vtable shared by all entities of that PMC type.\n\n> No, I mean why is the type-specific functionality not pushed down into\n> the next tier where it is actually needed, like the JVM and CTS do,\n> leaving the base PMC with only the same four or five methods those\n> systems have?\n\nI don't understand this question.\n\n> Without opening a can of bees, this sounds like Parrot's performance\n> will vary greatly, depending on the quantity of variables in scope in\n> subroutines.\n\nIndeed, in the same way that C's performance (especially during compilation) \ncan vary greatly, depending on the quantitiy of variables in scope in \nsubroutines.\n\n> While it is generally true for most languages that a \n> large number of variables can trigger load/store operations when the\n> register capacity is exceeded, Parrot will switch from JIT code to\n> purely interpreted code?\n\nPerhaps.  Perhaps not.  Unimplemented.\n\n> While most people don't worry about \n> incurring a few load/store operations, this kind of variation may\n> cause programmers to alter their programming style significantly in\n> order to avoid unacceptable performance.\n\nI doubt that.  By far most of the programs I've ever written (let alone seen) \nspend more time IO-bound than CPU bound.\n\n> As you say, i386 has fewer registers, but it is a very common\n> platform.  Given that, many programmers may consider it necessary to\n> write code that will be JIT-able on that platform, leading to a rather\n> awkward programming style, encouraging the use of a larger number of\n> subroutines, thus more calling, and ultimately a lot of register\n> shuffling anyway.\n\nI consider the use of smaller subroutines to offer orders of magnitude in \nbenefit with regard to maintainable code, far more than any potential gain or \nloss of performance.\n\nI suspect that line of thinking is prevalent among many other users of dynamic \nlanguages.\n\n> When I asked this question, I thought I was asking if the compiler\n> could suggest which variables should map to registers and which ones\n> should be loaded/stored.  But it seems this is a question of which\n> subroutines will use registers at all.  In that case, I wonder what\n> mechanisms Parrot will provide to inform a compiler how JIT-able a\n> subroutine is -- both on the current platform and on other\n> architectures -- to enable the compiler to know when it would make\n> sense to either automatically modify the code into JIT-able form, or\n> to warn the developer.\n\nUnspecified at the moment.\n\n> Frankly, this is not much of an answer.  I am not asking if CISC\n> architectures exist, but rather I am asking why you are choosing to\n> create one.\n\nInteroperability.\n\nTo some degree, optimization.\n\nIt's easier to optimize a reasonably simple, if specialized, operation than it \nis to optimize the same operation built out of many more, if simpler, \ncomponents.\n\nYou can certainly have only one \"add\" opcode in a system, and that's fine.  It \npops two arguments off of the stack, checks their types, and performs the \nright type of operation, deciding if it's an integral addition or a floating \npoint addition and what type of result to provide.\n\nOr you could have \"add_int_int_int\" (which takes two ints and produces an int) \nand \"add_float_int_float\" (which takes an int and a float and produces a \nfloat) and compile that code down to JITted instructions.\n\nThen get rid of the stack and you can dispatch to those ops without going \nthrough the thunk.\n\nThen use VM registers, and you always know how to map to and from the right \nCPU registers with the proper offsets into your call frame, and things might \nget even faster.\n\nThen again, no benchmarks; shouldn't speculate.\n\n> It is not sufficient to say that one can write the code.  How will\n> Parrot inform an existing compiler that the new operation exists (or\n> does not exist if the version of Parrot is older).\n\nI don't really understand the question.\n\n> Will compilers have to themselves be recompiled even if they do not use the \n> new operators?\n\nI don't understand this question either.  Can you give an example?\n\n> Also, this seems, as a design, to simply be a bag of operations.\n\nSo is Perl.  I find understanding Perl 5's design helpful in working on \nParrot.\n\n> Finally, I would like to add some additional questions.\n>\n> 2.h. Will Parrot support inline assembly language?\n\nDoubtful.\n\n> 2.i. Will Parrot support primitive types?\n\nIt already supports strings, floats, and integers.  Do you mean bits and \nchunks of memory x-bits wide?  There is support for that to some degree.\n\n> 4.c. How will registers benefit PMCs (e.g. PerlScalar), which are not\n> primitive types and cannot be stored in a hardware register?\n\nI don't understand which type of register you mean when you first use the \nword.  Additionally, I don't understand the question either way I could \ninterpret it.\n\n-- c\n\n"}