{"category": "ham", "to_address": "\"Bi-Info (http://members.home.nl/bi-info)\" <bi-info@home.nl>,\n   \"Gabor Grothendieck\" <ggrothendieck@gmail.com>", "from_address": "\"Greg Snow\" <Greg.Snow@intermountainmail.org>", "subject": "Re: [R] Reasons to Use R", "body": "> -----Original Message-----\n> From: r-help-bounces@stat.math.ethz.ch \n> [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of \n> Bi-Info (http://members.home.nl/bi-info)\n> Sent: Monday, April 09, 2007 4:23 PM\n> To: Gabor Grothendieck\n> Cc: Lorenzo Isella; r-help@stat.math.ethz.ch\n> Subject: Re: [R] Reasons to Use R\n\n[snip] \n\n> So what's the big deal about S using files instead of memory \n> like R. I don't get the point. Isn't there enough swap space \n> for S? (Who cares\n> anyway: it works, isn't it?) Or are there any problems with S \n> and large datasets? I don't get it. You use them, Greg. So \n> you might discuss that issue.\n> \n> Wilfred\n> \n> \n\nThis is my understanding of the issue (not anything official).\n\nIf you use up all the memory while in R, then the OS will start swapping\nmemory to disk, but the OS does not know what parts of memory correspond\nto which objects, so it is entirely possible that the chunk swapped to\ndisk contains parts of different data objects, so when you need one of\nthose objects again, everything needs to be swapped back in.  This is\nvery inefficient.\n\nS-PLUS occasionally runs into the same problem, but since it does some\nof its own swapping to disk it can be more efficient by swapping single\ndata objects (data frames, etc.).  Also, since S-PLUS is already saving\neverything to disk, it does not actually need to do a full swap, it can\njust look and see that a particular data frame has not been used for a\nwhile, know that it is already saved on the disk, and unload it from\nmemory without having to write it to disk first.\n\nThe g.data package for R has some of this functionality of keeping data\non the disk until needed.\n\nThe better approach for large data sets is to only have some of the data\nin memory at a time and to automatically read just the parts that you\nneed.  So for big datasets it is recommended to have the actual data\nstored in a database and use one of the database connection packages to\nonly read in the subset that you need.  The SQLiteDF package for R is\nworking on automating this process for R.  There are also the bigdata\nmodule for S-PLUS and the biglm package for R have ways of doing some of\nthe common analyses using chunks of data at a time.  This idea is not\nnew.  There was a program in the late 1970s and 80s called Rummage by\nDel Scott (I guess technically it still exists, I have a copy on a 5.25\"\nfloppy somewhere) that used the approach of specify the model you wanted\nto fit first, then specify the data file.  Rummage would then figure out\nwhich sufficient statistics were needed and read the data in chunks,\ncompute the sufficient statistics on the fly, and not keep more than a\ncouple of lines of the data in memory at once.  Unfortunately it did not\nhave much of a user interface, so when memory was cheap and datasets\nonly medium sized it did not compete well, I guess it was just a bit too\nahead of its time.\n\nHope this helps, \n\n\n\n-- \nGregory (Greg) L. Snow Ph.D.\nStatistical Data Center\nIntermountain Healthcare\ngreg.snow@intermountainmail.org\n(801) 408-8111\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}