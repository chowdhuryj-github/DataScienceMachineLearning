{"category": "ham", "to_address": "j-simpson@northwestern.edu", "from_address": "Roland Rau <roland.rproject@gmail.com>", "subject": "Re: [R] size limit in R?", "body": "j-simpson@northwestern.edu wrote:\n > Hi,\n >\n > Please see the email exchanges below.  I am having trouble generating \noutput that is large enough\n > for our needs, specifically when using the GaussRF function. \nHowever, when I wrote Dr. Schlather\n > (the author of the GaussRF function), he indicated that there is also \na limit imposed by R itself.\n > Is this something that we can overcome?\n >\n\nI could be wrong, but I think you did not provide information on your \nplatform. Assuming it is Win32, it is an FAQ, please see:\nhttp://cran.r-project.org/bin/windows/base/rw-FAQ.html#There-seems-to-be-a-limit-on-the-memory-it-uses_0021\n\n\n >\n >> x <- numeric( 200 / 0.025 * 1450 / 0.025)\n > Error: cannot allocate vector of size 3625000 Kb\n >\n\nYou will hit memory limits rather quickly if you want to allocate not \nonly one of your 3.6GB vectors - and this is neither the fault of R nor \nof Win32.\nAlthough I don't have a background in Computer Science, I think the \nphysical limit to address memory on a 32bit system is 4GB.\n > 2^32/(1024*1024*1024)\n[1] 4\n\n\nI hope this helps?\nRoland\n\n(And I hope I did not claim anything wrong about 32bit systems)\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}