{"category": "ham", "to_address": "Ravi Varadhan <rvaradhan@jhmi.edu>", "from_address": "DEEPANKAR BASU <basu.15@osu.edu>", "subject": "Re: [R] Estimates at each iteration of optim()?", "body": "Thanks a lot for all your suggestions; they have been extremely helpful. I will work through each (starting with Ravi's suggestions) and get back with other questions if they arise.\n\nDeepankar\n\n----- Original Message -----\nFrom: Ravi Varadhan \nDate: Monday, April 23, 2007 1:26 pm\nSubject: Re: [R] Estimates at each iteration of optim()?\n\n> Without knowing much about your problem, it is hard to suggest good\n> strategies.  However, if you are having trouble with the estimates of\n> covariance matrix not being positive-definite, you can force them \n> to be\n> positive-definite after each iteration, before moving on to the next\n> iteration.  Look at the \"make.positive.definite\" function from \n> \"corpcor\"package.  This is just one approach. There may be better \n> approaches -\n> perhaps, an EM-like approach might be applicable that would \n> automaticallysatisfy all parameter constraints.\n> \n> Ravi.\n> \n> --------------------------------------------------------------------\n> --------\n> -------\n> \n> Ravi Varadhan, Ph.D.\n> \n> Assistant Professor, The Center on Aging and Health\n> \n> Division of Geriatric Medicine and Gerontology \n> \n> Johns Hopkins University\n> \n> Ph: (410) 502-2619\n> \n> Fax: (410) 614-9625\n> \n> Email: rvaradhan@jhmi.edu\n> \n> Webpage:  \n> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html\n> \n> \n> --------------------------------------------------------------------\n> --------\n> --------\n> \n> -----Original Message-----\n> From: DEEPANKAR BASU [basu.15@osu.edu] \n> Sent: Monday, April 23, 2007 1:09 PM\n> To: Ravi Varadhan\n> Cc: 'Peter Dalgaard'; r-help@stat.math.ethz.ch\n> Subject: Re: RE: [R] Estimates at each iteration of optim()?\n> \n> Ravi,\n> \n> Thanks a lot for your detailed reply. It clarifies many of the \n> confusions in\n> my mind. \n> \n> I want to look at the parameter estimates at each iteration because \n> the full\n> model that I am trying to estimate is not converging; a smaller \n> version of\n> the model converges but the results are quite meaningless. The \n> problem in\n> the estimation of the full model is the following: my likelihood \n> functioncontains the elements of a (bivariate normal) covariance \n> matrix as\n> parameters. To compute the likelihood, I have to draw random \n> samples from\n> the bivariate normal distribution. But no matter what starting \n> values I\n> give, I cannot ensure that the covariance matrix remains positive \n> definiteat each iteration of the optimization exercise. Moreover, \n> as soon as the\n> covariance matrix fails to be positive definite, I get an error \n> message(because I can no longer draw from the bivariate normal \n> distribution) and\n> the program stops. Faced with this problem, I wanted to see exactly \n> at which\n> parameter estimates the covariance matrix fails to remain positive \n> definite.>From that I would think of d\n> evising a method to get around the problem, at least I would try \n> to.  \n> \n> Probably there is some other way to solve this problem. I would \n> like your\n> opinion on the following question: is there some way I can \n> transform the\n> three parametrs of my (2 by 2) covariance matrix (the two standard\n> devaitions and the correlation coefficient) to ensure that the \n> covariancematrix remains positive definite at each iteration of the \n> optimization. Is\n> there any method other than transforming the parameters to ensure \n> this?\n> Deepankar\n> \n> \n> \n> ----- Original Message -----\n> From: Ravi Varadhan \n> Date: Monday, April 23, 2007 12:21 pm\n> Subject: RE: [R] Estimates at each iteration of optim()?\n> \n> > Deepankar,\n> > \n> > Here is an example using BFGS:\n> > \n> > > fr <- function(x) {   ## Rosenbrock Banana function\n> > +     x1 <- x[1]\n> > +     x2 <- x[2]\n> > +     100 * (x2 - x1 * x1)^2 + (1 - x1)^2\n> > + }\n> > > grr <- function(x) { ## Gradient of 'fr'\n> > +     x1 <- x[1]\n> > +     x2 <- x[2]\n> > +     c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),\n> > +        200 *      (x2 - x1 * x1))\n> > + }\n> > > optim(c(-1.2,1), fr, grr, method = \"BFGS\", \n> control=list(trace=TRUE))> initial  value 24.200000 \n> > iter  10 value 1.367383\n> > iter  20 value 0.134560\n> > iter  30 value 0.001978\n> > iter  40 value 0.000000\n> > final  value 0.000000 \n> > converged\n> > $par\n> > [1] 1 1\n> > \n> > $value\n> > [1] 9.594955e-18\n> > \n> > $counts\n> > function gradient \n> >     110       43 \n> > \n> > $convergence\n> > [1] 0\n> > \n> > $message\n> > NULL\n> > \n> > > \n> > \n> > This example shows that the parameter estimates are printed out \n> > every 10\n> > iterations.  However, trying different integer values for trace \n> > from 2 to 10\n> > (trace = 1 behaves the same as trace=TRUE) did not change \n> anything. \n> > If you\n> > want to get estimates at every iteration, look at the source code \n> > for BFGS\n> > (which I assume is in FORTRAN). You may have to modify the source \n> > code and\n> > recompile it yourself to get more detailed trace for BFGS. \n> > \n> > However, you can get parameter iterates at every step for \"L-BFGS-\n> > B\" using\n> > trace=6, although this gives a lot more information than just the \n> > parameterestimates.  Alternatively, you can use the \"CG\" methods \n> > with trace=TRUE or\n> > trace=1, which is a generally a lot slower than BFGS or L-BFGS-B.\n> > \n> > Why do you want to look at parameter estimates for each step, \n> anyway?> \n> > \n> > Ravi.\n> > \n> > ------------------------------------------------------------------\n> --\n> > --------\n> > -------\n> > \n> > Ravi Varadhan, Ph.D.\n> > \n> > Assistant Professor, The Center on Aging and Health\n> > \n> > Division of Geriatric Medicine and Gerontology \n> > \n> > Johns Hopkins University\n> > \n> > Ph: (410) 502-2619\n> > \n> > Fax: (410) 614-9625\n> > \n> > Email: rvaradhan@jhmi.edu\n> > \n> > Webpage:  \n> > http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html\n> > \n> > \n> > ------------------------------------------------------------------\n> --\n> > --------\n> > --------\n> > \n> > -----Original Message-----\n> > From: r-help-bounces@stat.math.ethz.ch\n> > [r-help-bounces@stat.math.ethz.ch] On Behalf Of DEEPANKAR BASU\n> > Sent: Monday, April 23, 2007 11:34 AM\n> > To: Peter Dalgaard\n> > Cc: r-help@stat.math.ethz.ch\n> > Subject: Re: [R] Estimates at each iteration of optim()?\n> > \n> > I read the description of the trace control parameter in ?optim \n> and \n> > thenalso looked at the examples given at the end. In one of the \n> > examples I found\n> > that they had used \"trace=TRUE\"  with the method \"SANN\". I am \n> using \n> > themethod \"BFGS\" and I tried using \"trace=TRUE\" too but I did not \n> > get the\n> > parameter estimates at each iteration. As you say, it might be \n> method> dependent. I tried reading the source code for \"optim\" but \n> could \n> > not find\n> > out what I was looking for. Hence, I was wondering if anyone \n> could \n> > tell me\n> > what option to use with the method \"BFGS\" to get the parameter \n> > estimates at\n> > each iteration of the optimization.\n> > \n> > Deepankar\n> > \n> > \n> > ----- Original Message -----\n> > From: Peter Dalgaard \n> > Date: Monday, April 23, 2007 2:46 am\n> > Subject: Re: [R] Estimates at each iteration of optim()?\n> > \n> > > DEEPANKAR BASU wrote:\n> > > > I am trying to maximise a complicated loglikelihood function \n> > with \n> > > the \"optim\" command. Is there some way to get to know the \n> > estiamtes \n> > > at each iteration? When I put \"control=list(trace=TRUE)\" as an \n> > > option in \"optim\", I just got the initial and final values of \n> the \n> > > loglikelihood, number of iterations and whether the routine has \n> > > converged or not. I need to know the estimate values at each \n> > > iteration.>\n> > > >   \n> > > It might help if you actually _read_ the description of the \n> trace \n> > > control parameter (hint: it is not an on/off switch) in \n> ?optim... \n> > > And, \n> > > as it says, this is method dependent, so you may have to study \n> > the \n> > > source code.\n> > > \n> > > > Deepankar\n> > > >\n> > > > ______________________________________________\n> > > > R-help@stat.math.ethz.ch mailing list\n> > > > https://stat.ethz.ch/mailman/listinfo/r-help\n> > > > PLEASE do read the posting guide http://www.R-\n> > project.org/posting-\n> > > guide.html> and provide commented, minimal, self-contained, \n> > > reproducible code.\n> > > >   \n> > > \n> > >\n> > \n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide http://www.R-project.org/posting-\n> > guide.htmland provide commented, minimal, self-contained, \n> > reproducible code.\n> >\n> \n> ______________________________________________\n> R-help@stat.math.ethz.ch mailing list\n> https://stat.ethz.ch/mailman/listinfo/r-help\n> PLEASE do read the posting guide http://www.R-project.org/posting-\n> guide.htmland provide commented, minimal, self-contained, \n> reproducible code.\n> >>> This e-mail and any attachments are confidential, may contain \n> legal, professional or other privileged information, and are \n> intended solely for the addressee.  If you are not the intended \n> recipient, do not use the information in this e-mail in any way, \n> delete this e-mail and notify the sender. CEG-IP1\n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}