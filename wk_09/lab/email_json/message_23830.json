{"category": "ham", "to_address": "\"Prof. Jeffrey Cardille\" <jeffrey.cardille@umontreal.ca>", "from_address": "Frank E Harrell Jr <f.harrell@vanderbilt.edu>", "subject": "Re: [R] pseudo-R2 or GOF for regression trees?", "body": "Prof. Jeffrey Cardille wrote:\n> Hello,\n> \n> Is there an accepted way to convey, for regression trees, something  \n> akin to R-squared?\n> \n> I'm developing regression trees for a continuous y variable and I'd  \n> like to say how well they are doing. In particular, I'm analyzing the  \n> results of a simulation model having highly non-linear behavior, and  \n> asking what characteristics of the inputs are related to a particular  \n> output measure.  I've got a very large number of points: n=4000.  I'm  \n> not able to do a model sensitivity analysis because of the large  \n> number of inputs and the model run time.\n> \n> I've been googling around both on the archives and on the rest of the  \n> web for several hours, but I'm still having trouble getting a firm  \n> sense of the state of the art.  Could someone help me to quickly  \n> understand what strategy, if any, is acceptable to say something like  \n> \"The regression tree in Figure 3 captures 42% of the variance\"?  The  \n> target audience is readers who will be interested in the subsequent  \n> verbal explanation of the relationship, but only once they are  \n> comfortable that the tree really does capture something.  I've run  \n> across methods to say how well a tree does relative to a set of trees  \n> on the same data, but that doesn't help much unless I'm sure the  \n> trees in question are really capturing the essence of the system.\n> \n> I'm happy to be pointed to a web site or to a thread I may have  \n> missed that answers this exact question.\n> \n> Thanks very much,\n> \n> Jeff\n> \n> ------------------------------------------\n> Prof. Jeffrey Cardille\n> jeffrey.cardille@umontreal.ca\n> R-help@stat.math.ethz.ch mailing list\n\nYe (below) has a method to get a nearly unbiased estimate of R^2 from \nrecursive partitioning.  In his examples the result was similar to using \nthe formula for adjusted R^2 with regression degrees of freedom equal to \nabout 3n/4.  You can also use something like 10-fold cross-validation \nrepeated 20 times to get a fairly precise and unbiased estimate of R^2.\n\nFrank\n\n\n>@ARTICLE{ye98mea,\n   author = {Ye, Jianming},\n   year = 1998,\n   title = {On measuring and correcting the effects of data mining and model\n           selection},\n   journal = JASA,\n   volume = 93,\n   pages = {120-131},\n   annote = {generalized degrees of freedom;GDF;effective degrees of\n            freedom;data mining;model selection;model\n            uncertainty;overfitting;nonparametric regression;CART;simulation\n            setup}\n}\n-- \nFrank E Harrell Jr   Professor and Chair           School of Medicine\n                      Department of Biostatistics   Vanderbilt University\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}