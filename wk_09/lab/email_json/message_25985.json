{"category": "ham", "to_address": "\"John Fox\" <jfox@mcmaster.ca>", "from_address": "\"hadley wickham\" <h.wickham@gmail.com>", "subject": "Re: [R] Weighted least squares", "body": "Thanks John,\n\nThat's just the explanation I was looking for. I had hoped that there\nwould be a built in way of dealing with them with R, but obviously\nnot.\n\nGiven that explanation, it stills seems to me that the way R\ncalculates n is suboptimal, as demonstrated by my second example:\n\nsummary(lm(y ~ x, data=df, weights=rep(c(0,2), each=50)))\nsummary(lm(y ~ x, data=df, weights=rep(c(0.01,2), each=50)))\n\nthe weights are only very slightly different but the estimates of\nresidual standard error are quite different (20 vs 14 in my run)\n\nHadley\n\nOn 5/8/07, John Fox  wrote:\n> Dear Hadley,\n>\n> I think that the problem is that the term \"weights\" has different meanings,\n> which, although they are related, are not quite the same.\n>\n> The weights used by lm() are (inverse-)\"variance weights,\" reflecting the\n> variances of the errors, with observations that have low-variance errors\n> therefore being accorded greater weight in the resulting WLS regression.\n> What you have are sometimes called \"case weights,\" and I'm unaware of a\n> general way of handling them in R, although you could regenerate the\n> unaggregated data. As you discovered, you get the same coefficients with\n> case weights as with variance weights, but different standard errors.\n> Finally, there are \"sampling weights,\" which are inversely proportional to\n> the probability of selection; these are accommodated by the survey package.\n>\n> To complicate matters, this terminology isn't entirely standard.\n>\n> I hope this helps,\n>  John\n>\n> --------------------------------\n> John Fox, Professor\n> Department of Sociology\n> McMaster University\n> Hamilton, Ontario\n> Canada L8S 4M4\n> 905-525-9140x23604\n> http://socserv.mcmaster.ca/jfox\n> --------------------------------\n>\n> > -----Original Message-----\n> > From: r-help-bounces@stat.math.ethz.ch\n> > [mailto:r-help-bounces@stat.math.ethz.ch] On Behalf Of hadley wickham\n> > Sent: Tuesday, May 08, 2007 5:09 AM\n> > To: R Help\n> > Subject: [R] Weighted least squares\n> >\n> > Dear all,\n> >\n> > I'm struggling with weighted least squares, where something\n> > that I had assumed to be true appears not to be the case.\n> > Take the following data set as an example:\n> >\n> > df <- data.frame(x = runif(100, 0, 100)) df$y <- df$x + 1 +\n> > rnorm(100, sd=15)\n> >\n> > I had expected that:\n> >\n> > summary(lm(y ~ x, data=df, weights=rep(2, 100))) summary(lm(y\n> > ~ x, data=rbind(df,df)))\n> >\n> > would be equivalent, but they are not.  I suspect the\n> > difference is how the degrees of freedom is calculated - I\n> > had expected it to be sum(weights), but seems to be\n> > sum(weights > 0).  This seems unintuitive to me:\n> >\n> > summary(lm(y ~ x, data=df, weights=rep(c(0,2), each=50)))\n> > summary(lm(y ~ x, data=df, weights=rep(c(0.01,2), each=50)))\n> >\n> > What am I missing?  And what is the usual way to do a linear\n> > regression when you have aggregated data?\n> >\n> > Thanks,\n> >\n> > Hadley\n> >\n> > ______________________________________________\n> > R-help@stat.math.ethz.ch mailing list\n> > https://stat.ethz.ch/mailman/listinfo/r-help\n> > PLEASE do read the posting guide\n> > http://www.R-project.org/posting-guide.html\n> > and provide commented, minimal, self-contained, reproducible code.\n> >\n>\n>\n>\n\n______________________________________________\nR-help@stat.math.ethz.ch mailing list\nhttps://stat.ethz.ch/mailman/listinfo/r-help\nPLEASE do read the posting guide http://www.R-project.org/posting-guide.html\nand provide commented, minimal, self-contained, reproducible code.\n\n"}